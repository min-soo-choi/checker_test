# app.py
# -*- coding: utf-8 -*-
import json
import time
import re
import html
from collections import Counter
from typing import Dict, Any, List

import streamlit as st
import google.generativeai as genai


# --------------------------
# 0. Gemini ì„¤ì • (í‚¤ëŠ” secretsì—ì„œë§Œ ì½ê¸°)
# --------------------------
API_KEY = st.secrets.get("GEMINI_API_KEY")
if not API_KEY:
    st.error("GEMINI_API_KEYê°€ secretsì— ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
    st.stop()

genai.configure(api_key=API_KEY)
model = genai.GenerativeModel("gemini-2.0-flash-001")


# -------------------------------------------------
# ê³µí†µ ìœ í‹¸
# -------------------------------------------------

# í•œ chunkë‹¹ ìµœëŒ€ ê¸¸ì´ (ì›í•˜ëŠ” ê°’ìœ¼ë¡œ ì¡°ì • ê°€ëŠ¥)
MAX_KO_CHUNK_LEN = 1000  # í•œê¸€ 800~1200ì ì •ë„ë©´ ì•ˆì •ì 

def split_korean_text_into_chunks(text: str, max_len: int = MAX_KO_CHUNK_LEN) -> List[str]:
    """
    ê¸´ í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ì—¬ëŸ¬ chunkë¡œ ë‚˜ëˆˆë‹¤.
    - ê¸°ë³¸ ê¸°ì¤€: max_len ê¸€ì
    - ê°€ëŠ¥í•˜ë©´ ì¤„ë°”ê¿ˆ(\n) ì•ì—ì„œ ëŠì–´ì„œ ë¬¸ë‹¨ ë‹¨ìœ„ì— ê°€ê¹ê²Œ ìœ ì§€
    """
    if not text:
        return []

    text = text.replace("\r\n", "\n")
    if len(text) <= max_len:
        return [text]

    chunks: List[str] = []
    n = len(text)
    start = 0

    while start < n:
        end = min(start + max_len, n)

        # end ê·¼ì²˜ì—ì„œ ì¤„ë°”ê¿ˆ ê¸°ì¤€ìœ¼ë¡œ ëŠì„ ìˆ˜ ìˆìœ¼ë©´ ê±°ê¸°ì„œ ëŠê¸°
        split_pos = text.rfind("\n", start + int(max_len * 0.4), end)
        if split_pos == -1 or split_pos <= start:
            split_pos = end

        chunk = text[start:split_pos].strip("\n")
        if chunk:
            chunks.append(chunk)

        start = split_pos

    return chunks

# -------------------------------------------------
# PDF í…ìŠ¤íŠ¸ ì •ë¦¬ìš© í”„ë¡¬í”„íŠ¸ + ë˜í¼
# -------------------------------------------------

PDF_RESTORE_SYSTEM_PROMPT = """
ë„ˆëŠ” PDFì—ì„œ ë³µì‚¬í•´ ë¶™ì—¬ë„£ì€ í•œêµ­ì–´ ì‹œí—˜ì§€/í•´ì„¤ í…ìŠ¤íŠ¸ë¥¼,
ì›ë¬¸ì˜ ì˜ë¯¸ë¥¼ ìœ ì§€í•˜ë©´ì„œ êµ¬ì¡°ì™€ ì„œì‹ì„ ì •ë¦¬í•´ ì£¼ëŠ” ë„ìš°ë¯¸ì´ë‹¤.
ì•„ë˜ ê·œì¹™ì„ ìˆœì„œëŒ€ë¡œ, ì—„ê²©í•˜ê²Œ ì§€ì¼œë¼.

1. í…ìŠ¤íŠ¸ ë³µì› ë° ì •ë¹„
- ì˜¤íƒ€ ë° ê¹¨ì§„ ê¸€ì ë³µì›:
  ì…ë ¥ëœ í…ìŠ¤íŠ¸ì—ì„œ OCR ì˜¤ë¥˜ë¡œ ë³´ì´ëŠ” ê¹¨ì§„ ë¬¸ì(ì˜ˆ: ï€€, á†¢)ë‚˜ ëª…ë°±í•œ ì˜¤íƒ€
  (ì˜ˆ: ì—°ê³µ ì§€ëŠ¥ â†’ ì¸ê³µ ì§€ëŠ¥)ë¥¼ ë¬¸ë§¥ì— ë§ê²Œ ì˜¬ë°”ë¥¸ í•œê¸€, í•œì, ë¬¸ì¥ë¶€í˜¸ë¡œ ë³µì›í•œë‹¤.
- ì›ë¬¸ ìœ ì§€:
  í…ìŠ¤íŠ¸ì˜ ë‚´ìš©ì„ ì„ì˜ë¡œ ì°½ì‘í•˜ê±°ë‚˜ ì™œê³¡í•˜ì§€ ë§ê³ , ì›ë¬¸ì˜ ì˜ë¯¸ë¥¼ ê·¸ëŒ€ë¡œ ë³´ì¡´í•œë‹¤.

2. í—¤ë”(ì œëª©) í…ìŠ¤íŠ¸ ë³€ê²½ ê·œì¹™ (ì¤‘ìš”)
í…ìŠ¤íŠ¸ ë‚´ì˜ ë‹¤ìŒ í‚¤ì›Œë“œë“¤ì„ ì°¾ì•„ ì§€ì •ëœ í‘œì¤€ í—¤ë”ë¡œ ë³€ê²½í•œë‹¤.

[ì •ë‹µ í•´ì„¤]
- ì •ë‹µ
- ì •ë‹µì¸ ì´ìœ 
- ( ) ì •ë‹µì¸ ì´ìœ 
- ì •ë‹µ í•´ì„¤
- ì •ë‹µ ì„¤ëª…
- í•´ì„¤
- [ ] í•´ì„¤
- í•´ì„¤:
â€» â€˜í•´ì„¤â€™ ê´€ë ¨ í‘œí˜„ì€ ëª¨ë‘ [ì •ë‹µ í•´ì„¤]ë¡œ í†µí•©

[ì˜¤ë‹µ í•´ì„¤]
- ì˜¤ë‹µ
- ì˜¤ë‹µ í•´ì„¤
- ì˜¤ë‹µ í’€ì´
- ( ) ì˜¤ë‹µ í•´ì„¤
- ( ) í•´ì„¤ (ë¬¸ë§¥ìƒ ì˜¤ë‹µ í’€ì´ì¼ ê²½ìš°)

[ì ì ˆí•˜ì§€ ì•Šì€ ì´ìœ ]
- âœ ì ì ˆí•˜ì§€ ì•Šì€ ì´ìœ 
â€» í™”ì‚´í‘œ(âœ)ê°€ ìˆëŠ” ê²½ìš°

[ì ì ˆí•œ ì´ìœ ]
- âœ ì ì ˆí•œ ì´ìœ 
â€» í™”ì‚´í‘œ(âœ)ê°€ ì—†ëŠ” ê²½ìš°

[ì¶œì œ ì˜ë„]
- ì¶œì œ ì˜ë„
- ì¶œì œì˜ë„
â€» ê´„í˜¸ë§Œ [] í˜•íƒœë¡œ ë³€ê²½

[ì¤‘ì„¸ì˜ë„]
- ì¤‘ì„¸ì˜ë„
â€» ê´„í˜¸ë§Œ [] í˜•íƒœë¡œ ë³€ê²½

3. í—¤ë” ìˆœì„œ ì¬ë°°ì¹˜ (êµ¬ì¡° êµì •)
- ë³€í™˜ ì‘ì—…ì„ ë§ˆì¹œ í›„, ë§Œì•½ [ì˜¤ë‹µ í•´ì„¤]ì´ [ì •ë‹µ í•´ì„¤]ë³´ë‹¤ ë¨¼ì € ë‚˜ì˜¤ëŠ” ê²½ìš°
  í…ìŠ¤íŠ¸ ë‚´ìš©ì€ ê·¸ëŒ€ë¡œ ë‘ê³  í—¤ë”ì˜ ìœ„ì¹˜ë§Œ ì„œë¡œ ë§ë°”ê¾¼ë‹¤.
- ëª©í‘œ ìˆœì„œ:
  ë°˜ë“œì‹œ [ì •ë‹µ í•´ì„¤] â†’ [ì˜¤ë‹µ í•´ì„¤] ìˆœì„œë¥¼ ìœ ì§€í•œë‹¤.
- í—¤ë” ë°”ë¡œ ì•„ë˜ì— ì˜¤ëŠ” ë³¸ë¬¸ ë‚´ìš©ë“¤ì€ í—¤ë”ì™€ í•¨ê»˜ ë¬¶ì–´ì„œ ì´ë™ì‹œí‚¨ë‹¤.

4. ë¬¸ì¥ ë° ì„œì‹ ì •ë¦¬ (ê°€ë…ì„± ìµœì í™”)
- ì¤„ë°”ê¿ˆ ë³‘í•©:
  ë¬¸ì¥ì˜ ì¤‘ê°„ì´ ì–´ìƒ‰í•˜ê²Œ ëŠê²¨ ìˆëŠ” ê²½ìš°, ì´ë¥¼ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°í•œë‹¤.
- ë²ˆí˜¸ ëª©ë¡ ë¶„ë¦¬:
  ë¬¸ì¥ ì¤‘ê°„ì´ë‚˜ ëì— ì› ë¬¸ì(â‘ , â‘¡, â‘¢â€¦ / ã‰ , ã‰¡â€¦)ê°€ ë¶™ì–´ ìˆëŠ” ê²½ìš°
  ë°˜ë“œì‹œ ì¤„ì„ ë°”ê¾¼ ë’¤ ë²ˆí˜¸ë¥¼ ì‹œì‘í•œë‹¤.
- ë¹ˆ ì¤„ ì œê±°:
  ë¶ˆí•„ìš”í•œ ë¹ˆ ì¤„(ì—”í„° ë‘ ë²ˆ ì´ìƒ)ì€ ì œê±°í•˜ê³ ,
  ë‹¨ì¼ ì¤„ë°”ê¿ˆ(ì—”í„° í•œ ë²ˆ)ë§Œ ì‚¬ìš©í•œë‹¤.

â€» ê°€ëŠ¥í•œ í•œ ê¸°ì¡´ í…ìŠ¤íŠ¸ì— ìˆë˜ ì›ê¸°í˜¸/ì„ ì§€ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë˜,
   ì¤„ ìœ„ì¹˜ì™€ ì¤„ë°”ê¿ˆë§Œ ì •ë¦¬í•œë‹¤.

5. ìµœì¢… ì¶œë ¥ í˜•ì‹
- ì™„ì„±ëœ í…ìŠ¤íŠ¸ëŠ” ë³µì‚¬í•˜ê¸° ì‰½ë„ë¡
  ë°˜ë“œì‹œ íšŒìƒ‰ ì½”ë“œ ë¸”ë¡(Code Block) ì•ˆì— ë‹´ì•„ì„œ ì¶œë ¥í•œë‹¤.
- ì½”ë“œ ë¸”ë¡ ë°–ì—ëŠ” ì–´ë–¤ ì„¤ëª…ë„ ì¶œë ¥í•˜ì§€ ë§ê³ ,
  ì˜¤ì§ ì •ë¦¬ëœ í…ìŠ¤íŠ¸ë§Œ ì½”ë“œ ë¸”ë¡ ì•ˆì— ë„£ì–´ë¼.
- ì½”ë“œ ë¸”ë¡ ì–¸ì–´ í‘œì‹œëŠ” textë¡œ ì‚¬ìš©í•´ë„ ë˜ê³ , ìƒëµí•´ë„ ëœë‹¤.

6) ë¸”ë¡ ê°„ ê³µë°± ê·œì¹™
- [ì •ë‹µ í•´ì„¤] ë¸”ë¡ê³¼ ê·¸ ë‹¤ìŒ ë¸”ë¡ ì‚¬ì´ì—ëŠ” ë¹ˆ ì¤„ì„ ì •í™•íˆ 1ì¤„ë§Œ ë‘”ë‹¤.
- [ì˜¤ë‹µ í•´ì„¤] ë¸”ë¡ê³¼ ì›ê¸°í˜¸(â‘ , â‘¡, ã‰ â€¦) ëª©ë¡ ì‚¬ì´ì—ë„ ë¹ˆ ì¤„ì„ ì •í™•íˆ 1ì¤„ë§Œ ë‘”ë‹¤.
- ë¸”ë¡ ë‚´ë¶€ì—ì„œëŠ” ë¶ˆí•„ìš”í•œ ì—°ì† ë¹ˆ ì¤„ì„ ì œê±°í•˜ê³  ë…¼ë¦¬ì ìœ¼ë¡œ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ë‹¨ì¼ ì¤„ë°”ê¿ˆì„ ìœ ì§€í•œë‹¤.

"""

def normalize_inline_answer_marker(text: str) -> str:
    """
    ë¬¸í•­ ë²ˆí˜¸ + ì •ë‹µ ê¸°í˜¸ê°€ ë¬¸ì¥ ì•ˆì— ì„ì—¬ ìˆëŠ” ê²½ìš°ë¥¼ ì •ê·œí™”í•œë‹¤.

    ì˜ˆ:
    "1) â‘£ ( ) ( ) ì¶œì œ ìœ í˜• ... [ì •ë‹µ í•´ì„¤] ..."
    â†’
    "1) ì •ë‹µ: â‘£\n[ì •ë‹µ í•´ì„¤] ..."
    """
    if not text:
        return text

    text = text.replace("\r\n", "\n")

    # â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©
    circled_nums = "â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©"

    # ë¬¸í•­ ë²ˆí˜¸ + ì •ë‹µ ê¸°í˜¸ íŒ¨í„´
    pattern = re.compile(
        rf"""
        (\b\d+\))            # 1) ê°™ì€ ë¬¸í•­ ë²ˆí˜¸
        \s*
        ([{circled_nums}])   # â‘£ ê°™ì€ ì •ë‹µ ê¸°í˜¸
        .*?
        (?=\[ì •ë‹µ\s*í•´ì„¤\])  # [ì •ë‹µ í•´ì„¤] ì§ì „ê¹Œì§€ë§Œ ë¨¹ìŒ
        """,
        re.VERBOSE | re.DOTALL,
    )

    def repl(m):
        qno = m.group(1)
        ans = m.group(2)
        return f"{qno} ì •ë‹µ: {ans}\n"

    return pattern.sub(repl, text)


def tighten_between_answer_blocks(text: str) -> str:
    """
    [ì •ë‹µ í•´ì„¤] ë¸”ë¡ê³¼ [ì˜¤ë‹µ í•´ì„¤] í—¤ë” ì‚¬ì´ì— ë“¤ì–´ê°„
    'ë¹ˆ ì¤„ 1ì¤„(ë˜ëŠ” ì—¬ëŸ¬ ì¤„)'ì„ ì œê±°í•´ì„œ ë°”ë¡œ ë¶™ì¸ë‹¤.

    ì˜ˆ)
    [ì •ë‹µ í•´ì„¤]
    í•´ì„¤ ë‚´ìš©

    [ì˜¤ë‹µ í•´ì„¤]

    â†’ [ì •ë‹µ í•´ì„¤]
      í•´ì„¤ ë‚´ìš©
      [ì˜¤ë‹µ í•´ì„¤]
    """
    if not text:
        return text

    # '\n(ë¹ˆ ì¤„ë“¤)\n[ì˜¤ë‹µ í•´ì„¤]' íŒ¨í„´ì„ '\n[ì˜¤ë‹µ í•´ì„¤]'ë¡œ ë°”ê¿ˆ
    # \s* ë•Œë¬¸ì— ê³µë°±/íƒ­ì´ ì„ì—¬ ìˆì–´ë„ ê°™ì´ ì œê±°ë¨
    text = re.sub(r"\n\s*\n(\[ì˜¤ë‹µ í•´ì„¤\])", r"\n\1", text)
    return text

def restore_pdf_text(raw_text: str) -> str:
    """
    PDFì—ì„œ ë³µì‚¬í•œ ë‚œì¥íŒ í…ìŠ¤íŠ¸ë¥¼, ìœ„ ê·œì¹™ì— ë”°ë¼ ì •ë¦¬í•´ ë‹¬ë¼ê³  Geminiì— ìš”ì²­.
    - ì…ë ¥: ì›ë³¸ í…ìŠ¤íŠ¸
    - ì¶œë ¥: ëª¨ë¸ì´ ë°˜í™˜í•œ ë¬¸ìì—´ (ê°€ëŠ¥í•˜ë©´ ì½”ë“œ ë¸”ë¡ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
    """
    if not raw_text:
        return ""

    # ëª¨ë¸ì— ë„˜ê¸¸ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"""{PDF_RESTORE_SYSTEM_PROMPT}

----------------------------------------
ì•„ë˜ëŠ” PDFì—ì„œ ë³µì‚¬í•´ì˜¨ ì›ë³¸ í…ìŠ¤íŠ¸ì´ë‹¤.
ì´ í…ìŠ¤íŠ¸ë¥¼ ìœ„ ê·œì¹™ì— ë”°ë¼ ì •ë¦¬í•˜ë¼.
ë°˜ë“œì‹œ ì •ë¦¬ëœ ìµœì¢… í…ìŠ¤íŠ¸ë§Œ ì½”ë“œ ë¸”ë¡ ì•ˆì— ë„£ì–´ì„œ ì¶œë ¥í•  ê²ƒ.

[ì›ë³¸ í…ìŠ¤íŠ¸ ì‹œì‘]
{raw_text}
[ì›ë³¸ í…ìŠ¤íŠ¸ ë]
"""

    # ì´ ê¸°ëŠ¥ì€ JSONì´ ì•„ë‹ˆë¼ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¥¼ ê¸°ëŒ€í•˜ë¯€ë¡œ
    # response_mime_typeì€ ì§€ì •í•˜ì§€ ì•ŠëŠ”ë‹¤.
    response = model.generate_content(
        prompt,
        generation_config={"temperature": 0.0},
    )
    text = getattr(response, "text", "") or ""
    stripped = text.strip()

    # ì½”ë“œë¸”ë¡ ì•ˆ/ë°–ì„ ì²˜ë¦¬í•˜ê¸° ì „ì—, ë‚´ìš© ë¶€ë¶„ ë¨¼ì € ì •ë¦¬
    # 1) ì½”ë“œë¸”ë¡ì´ë©´ ì•ˆìª½ë§Œ êº¼ë‚´ì„œ ê°€ê³µ
    m = re.match(r"^```[^\n]*\n(.*)\n```$", stripped, re.S)
    if m:
        inner = m.group(1)
        inner = normalize_inline_answer_marker(inner)
        inner = tighten_between_answer_blocks(inner)
        stripped = f"```text\n{inner}\n```"
    else:
        # ì½”ë“œë¸”ë¡ì´ ì•„ë‹ˆë¼ë©´ ìš°ë¦¬ê°€ ê°ì‹¸ì£¼ë©´ì„œ ì •ë¦¬
        inner = tighten_between_answer_blocks(stripped)
        inner = normalize_inline_answer_marker(inner)
        stripped = f"```text\n{inner}\n```"

    return stripped

def remove_first_line_in_code_block(block: str) -> str:
    """
    ```text
    AAA
    BBB
    CCC
    ```
    ì´ëŸ° ë¬¸ìì—´ì—ì„œ AAA ì¤„ë§Œ ì§€ìš°ê³ 

    ```text
    BBB
    CCC
    ```
    ë¡œ ëŒë ¤ì¤€ë‹¤.
    ì½”ë“œë¸”ë¡ì´ ì•„ë‹ˆì–´ë„ ê·¸ëƒ¥ ì²« ì¤„ë§Œ ì œê±°í•´ì„œ ë°˜í™˜.
    """
    if not block:
        return block

    stripped = block.strip()

    # 1) ì½”ë“œë¸”ë¡ í˜•íƒœì¸ì§€ ë¨¼ì € í™•ì¸
    m = re.match(r"^```[^\n]*\n(.*)\n```$", stripped, re.S)
    if m:
        inner = m.group(1)
    else:
        inner = stripped

    lines = inner.splitlines()
    if not lines:
        new_inner = ""
    else:
        # ì²« ì¤„ ì œê±°
        new_inner = "\n".join(lines[1:])

    # ì½”ë“œë¸”ë¡ì´ì—ˆë˜ ê²½ìš° ë‹¤ì‹œ ê°ì‹¸ì„œ ë°˜í™˜
    if m:
        return f"```text\n{new_inner}\n```"
    else:
        return new_inner




def _parse_report_with_pattern(source_text: str, report: str, pattern: re.Pattern[str]) -> List[Dict[str, Any]]:
    """
    ê³µìš© íŒŒì„œ: "- 'ì›ë¬¸' â†’ 'ìˆ˜ì •ì•ˆ': ì„¤ëª…" í¬ë§·ì„ ë°›ì•„ ìœ„ì¹˜ ì •ë³´ë¥¼ ê³„ì‚°í•œë‹¤.
    pattern: ì–¸ì–´ë³„ í—ˆìš© ë”°ì˜´í‘œ/í™”ì‚´í‘œë¥¼ ë°˜ì˜í•œ ì •ê·œì‹.
    """
    if not report:
        return []

    # ì›ë¬¸ í…ìŠ¤íŠ¸ë¥¼ í•œ ì¤„ì”© ìª¼ê°œê³ , ê° ì¤„ì˜ ì‹œì‘ offsetì„ ê¸°ë¡
    lines = source_text.splitlines(keepends=True)
    line_starts: List[int] = []
    offset = 0
    for ln in lines:
        line_starts.append(offset)
        offset += len(ln)

    def index_to_line_col(idx: int) -> tuple[int, int]:
        line_no = 1
        for i, start in enumerate(line_starts):
            if i + 1 < len(line_starts) and line_starts[i + 1] <= idx:
                line_no += 1
            else:
                break
        line_start_idx = line_starts[line_no - 1]
        col_no = idx - line_start_idx + 1
        return line_no, col_no

    results: List[Dict[str, Any]] = []

    for line in report.splitlines():
        s = line.strip()
        if not s:
            continue

        m = pattern.match(s)
        if not m:
            continue

        orig = m.group(1)
        fixed = m.group(2)
        msg = m.group(3)

        idx = source_text.find(orig)
        if idx == -1:
            results.append({
                "original": orig,
                "fixed": fixed,
                "message": msg,
                "line": None,
                "col": None,
            })
            continue

        line_no, col_no = index_to_line_col(idx)
        results.append({
            "original": orig,
            "fixed": fixed,
            "message": msg,
            "line": line_no,
            "col": col_no,
        })

    return results


def parse_korean_report_with_positions(source_text: str, report: str) -> List[Dict[str, Any]]:
    """
    í•œêµ­ì–´ìš© ë¦¬í¬íŠ¸ íŒŒì„œ
    - ê¸°ë³¸: '- "ì›ë¬¸" â†’ "ìˆ˜ì •ì•ˆ": ì„¤ëª…' í˜•ì‹
    - í—ˆìš©: ë”°ì˜´í‘œ ìœ ë¬´ ëª¨ë‘ í—ˆìš©, ìŠ¤ë§ˆíŠ¸ ë”°ì˜´í‘œ í—ˆìš©, ì¢…ê²°ë¶€í˜¸ ëˆ„ë½/ì—¬ë¶„ ë”°ì˜´í‘œë„ ê´€ëŒ€í•˜ê²Œ ë§¤ì¹­
    - í™”ì‚´í‘œëŠ” â†’ ë˜ëŠ” -> í—ˆìš©
    """
    patterns = [
        # 1) ì •ê·œ í¬ë§·: ì–‘ìª½ì— ë”°ì˜´í‘œ ìˆìŒ
        re.compile(
            r"""^-\s*['"â€œâ€â€˜â€™](.+?)['"â€œâ€â€˜â€™]\s*(?:â†’|->)\s*['"â€œâ€â€˜â€™](.+?)['"â€œâ€â€˜â€™]\s*:\s*(.+?)\s*['"â€œâ€â€˜â€™]?$""",
            re.UNICODE,
        ),
        # 2) ë”°ì˜´í‘œê°€ ì•„ì˜ˆ ì—†ëŠ” ê²½ìš°ë„ í—ˆìš©
        re.compile(
            r"""^-\s*(.+?)\s*(?:â†’|->)\s*(.+?)\s*:\s*(.+?)\s*['"â€œâ€â€˜â€™]?$""",
            re.UNICODE,
        ),
    ]

    for pat in patterns:
        results = _parse_report_with_pattern(source_text, report, pat)
        if results:
            return results

    return []


def parse_english_report_with_positions(source_text: str, report: str) -> List[Dict[str, Any]]:
    """
    ì˜ì–´ìš© ë¦¬í¬íŠ¸ íŒŒì„œ
    - í¬ë§·ì€ ë™ì¼í•˜ì§€ë§Œ ì˜ì–´ ì „ìš© ê·œì¹™ì„ ë¶„ë¦¬í•  ìˆ˜ ìˆë„ë¡ ë³„ë„ í•¨ìˆ˜ë¡œ ìœ ì§€
    """
    pattern = re.compile(
        r"""^-\s*['"â€œâ€â€˜â€™](.+?)['"â€œâ€â€˜â€™]\s*(?:â†’|->)\s*['"â€œâ€â€˜â€™](.+?)['"â€œâ€â€˜â€™]\s*:\s*(.+)$""",
        re.UNICODE,
    )
    return _parse_report_with_pattern(source_text, report, pattern)


# âœ… í•˜ìœ„ í˜¸í™˜: ê¸°ë³¸ íŒŒì„œëŠ” í•œêµ­ì–´ ê·œì¹™ìœ¼ë¡œ ë™ì‘
def parse_report_with_positions(source_text: str, report: str) -> List[Dict[str, Any]]:
    return parse_korean_report_with_positions(source_text, report)

def build_english_raw_report_for_highlight(raw_json: dict) -> str:
    """
    ì˜ì–´ raw_jsonì—ì„œ í•˜ì´ë¼ì´íŠ¸ìš© ë¦¬í¬íŠ¸ ë¬¸ìì—´ì„ ë§Œë“ ë‹¤.
    - two_pass_single_en ëª¨ë“œ: 1ì°¨ Detector ê¸°ì¤€ ë¦¬í¬íŠ¸ ì‚¬ìš© (ë” ê³¼ê²€ì¶œ)
    - ê·¸ ì™¸: content_typo_reportë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    """
    if not isinstance(raw_json, dict):
        return ""

    mode = raw_json.get("mode")

    if mode == "two_pass_single_en":
        draft = raw_json.get("initial_report_from_detector", "") or ""
        return draft.strip()

    # fallback: í˜¹ì‹œ ëª¨ë“œë¥¼ ì•ˆ ì“´ ê²½ìš°
    return (raw_json.get("content_typo_report") or "").strip()




def build_korean_raw_report_for_highlight(raw_json: dict) -> str:
    """
    í•œêµ­ì–´ raw_jsonì—ì„œ í•˜ì´ë¼ì´íŠ¸ìš© ë¦¬í¬íŠ¸ ë¬¸ìì—´ì„ ë§Œë“ ë‹¤.
    - single block: raw_json["translated_typo_report"] ê·¸ëŒ€ë¡œ ì‚¬ìš©
    - chunked: ê° chunk.raw.translated_typo_reportë¥¼ ë¸”ë¡ í—¤ë”ì™€ í•¨ê»˜ ì´ì–´ë¶™ì„
    """
    if not isinstance(raw_json, dict):
        return ""

    # chunking ëª¨ë“œ
    if raw_json.get("mode") == "chunked":
        st.info("â€» í…ìŠ¤íŠ¸ê°€ ê¸¸ì–´ ì—¬ëŸ¬ ë¸”ë¡ìœ¼ë¡œ ë‚˜ë‰˜ì–´ ê²€ì‚¬ë˜ì—ˆìœ¼ë©°, \ 1ì°¨/2ì°¨ JSONì€ chunkë³„ raw ì •ë³´ë¡œë§Œ ì¡´ì¬í•©ë‹ˆë‹¤.")
    else:
        with st.expander("1ì°¨ Detector JSON (í•„ìš” ì‹œ)", expanded=False):
            st.json(raw_json.get("detector_clean", {}))
        with st.expander("2ì°¨ Judge JSON (í•„ìš” ì‹œ)", expanded=False):
            st.json(raw_json.get("judge_clean", {}))
        lines: List[str] = []
        for chunk in raw_json.get("chunks", []):
            idx = chunk.get("index")
            raw = chunk.get("raw") or {}
            report = (raw.get("translated_typo_report") or "").strip()
            if not report:
                continue
            if idx is not None:
                lines.append(f"# [ë¸”ë¡ {idx}]")
            lines.append(report)
        return "\n".join(lines)

    # ë‹¨ì¼ ë¸”ë¡ ëª¨ë“œ
    return (raw_json.get("translated_typo_report") or "").strip()

PUNCT_COLOR_MAP = {
    ".": "#fff3cd",  # ì—°ë…¸ë‘ (ì¢…ê²°ë¶€í˜¸)
    "?": "#f8d7da",  # ì—°ë¶„í™ (ë¬¼ìŒí‘œ)
    "!": "#f5c6cb",  # ì—°í•œ ë¹¨ê°• (ëŠë‚Œí‘œ)
    ",": "#d1ecf1",  # ì—°í•˜ëŠ˜ (ì‰¼í‘œ)
    ";": "#d6d8d9",  # íšŒìƒ‰ í†¤ (ì„¸ë¯¸ì½œë¡ )
    ":": "#d6d8d9",  # íšŒìƒ‰ í†¤ (ì½œë¡ )
    '"': "#e0f7e9",  # ì—°ì—°ë‘ (ìŒë”°ì˜´í‘œ)
    "â€œ": "#e0f7e9",
    "â€": "#e0f7e9",
    "'": "#fce9d9",  # ì—°ì‚´êµ¬ (ì‘ì€ë”°ì˜´í‘œ)
    "â€˜": "#fce9d9",
    "â€™": "#fce9d9",
}

PUNCT_GROUPS: dict[str, set[str]] = {
    "ì¢…ê²°ë¶€í˜¸(.)": {"."},
    "ë¬¼ìŒí‘œ(?)": {"?"},
    "ëŠë‚Œí‘œ(!)": {"!"},
    "ì‰¼í‘œ(,)": {","},
    "ìŒë”°ì˜´í‘œ": {'"', "â€œ", "â€"},
    "ì‘ì€ë”°ì˜´í‘œ": {"'", "â€˜", "â€™"},
}

# í•œêµ­ì–´/ì˜ì–´ì—ì„œ ìì£¼ ì“°ëŠ” ë¬¸ì¥ë¶€í˜¸ ì„¸íŠ¸
PUNCT_CHARS = set(PUNCT_COLOR_MAP.keys()) | set([
    # í°ë”°ì˜´í‘œ/ì‘ì€ë”°ì˜´í‘œ
    '"', "'", "â€œ", "â€", "â€˜", "â€™",
    # ê´„í˜¸ë¥˜
    "(", ")", "[", "]", "{", "}",
    "ã€Œ", "ã€", "ã€", "ã€", "ã€ˆ", "ã€‰", "ã€Š", "ã€‹",
    # ê¸°íƒ€
    "â€¦", "Â·",
])


def highlight_text_with_spans(
    source_text: str,
    spans: List[Dict[str, Any]],
    selected_punct_chars: set[str] | None = None,
) -> str:
    """
    spans: parse_report_with_positions() ê²°ê³¼.
    - spansì— í•´ë‹¹í•˜ëŠ” 'original' êµ¬ê°„ì€ <mark>...</mark> ë¡œ ê°ì‹¸ì„œ ì˜¤ë¥˜ í•˜ì´ë¼ì´íŠ¸.
    - ê·¸ ë°–ì˜ ì˜ì—­ì— ìˆëŠ” ë¬¸ì¥ë¶€í˜¸ëŠ” ê¸°í˜¸ë³„ë¡œ ìƒ‰ì„ ë‹¤ë¥´ê²Œ ì£¼ì–´ <span style="...">ë¡œ ê°ì‹¼ë‹¤.

    âš ï¸ ì„¤ê³„:
      - ì˜¤ë¥˜ êµ¬ê°„(<mark>) ì•ˆì˜ ë¬¸ì¥ë¶€í˜¸ëŠ” ì¶”ê°€ ìƒ‰ì¹  ì—†ì´ markë§Œ ì ìš© (ì´ë¯¸ ê°•í•œ í•˜ì´ë¼ì´íŠ¸).
      - ì˜¤ë¥˜ê°€ ì•„ë‹Œ ì˜ì—­ì˜ ë¬¸ì¥ë¶€í˜¸ë§Œ ìƒ‰ìƒ í•˜ì´ë¼ì´íŠ¸.
    """
    if not source_text:
        return ""

    # 1) ì˜¤ë¥˜ êµ¬ê°„ interval ê³„ì‚°
    intervals: List[tuple[int, int]] = []

    if spans:
        for span in spans:
            orig = span.get("original")
            if not orig:
                continue
            start = source_text.find(orig)
            if start == -1:
                continue
            end = start + len(orig)
            intervals.append((start, end))

    # intervalsê°€ ì—†ìœ¼ë©´, ì˜¤ë¥˜ëŠ” ì—†ê³  ë¬¸ì¥ë¶€í˜¸ë§Œ ìƒ‰ì¹ 
    if not intervals:
        result_parts: List[str] = []
        for ch in source_text:
            if ch in PUNCT_CHARS and (selected_punct_chars is None or ch in selected_punct_chars):
                color = PUNCT_COLOR_MAP.get(ch, "#e2e3e5")
                result_parts.append(
                    f"<span style='background-color: {color}; padding: 0 2px; font-weight: 700; font-size: 1.05em; border-radius: 2px;'>{html.escape(ch)}</span>"
                )
            else:
                result_parts.append(html.escape(ch))
        return "".join(result_parts)

    # 2) ì˜¤ë¥˜ interval ì •ë¦¬ (ê²¹ì¹˜ëŠ” êµ¬ê°„ ë³‘í•©)
    intervals.sort(key=lambda x: x[0])
    merged_intervals: List[tuple[int, int]] = []
    cur_start, cur_end = intervals[0]
    for s, e in intervals[1:]:
        if s <= cur_end:  # ê²¹ì¹˜ë©´ ë³‘í•©
            cur_end = max(cur_end, e)
        else:
            merged_intervals.append((cur_start, cur_end))
            cur_start, cur_end = s, e
    merged_intervals.append((cur_start, cur_end))

    # 3) í•œ ê¸€ìì”© ìˆœíšŒí•˜ë©° HTML ìƒì„±
    result_parts: List[str] = []
    idx = 0
    interval_idx = 0
    in_error = False
    cur_err_end = None

    while idx < len(source_text):
        # í˜„ì¬ ìœ„ì¹˜ê°€ ìƒˆë¡œìš´ ì˜¤ë¥˜ intervalì˜ ì‹œì‘ì¸ì§€ í™•ì¸
        if interval_idx < len(merged_intervals):
            start, end = merged_intervals[interval_idx]
        else:
            start, end = None, None

        if (not in_error) and (start is not None) and (idx == start):
            # ì˜¤ë¥˜ êµ¬ê°„ ì‹œì‘
            in_error = True
            cur_err_end = end
            result_parts.append("<mark style='background: #fff3a3; padding: 0 2px; font-weight: 700; font-size: 1.05em; border-radius: 2px;'>")

        ch = source_text[idx]

        if in_error:
            # ì˜¤ë¥˜ êµ¬ê°„ ì•ˆì—ì„œëŠ” ë¬¸ì¥ë¶€í˜¸ ìƒ‰ì¹  X, markë§Œ ì‚¬ìš©
            result_parts.append(html.escape(ch))
            idx += 1

            # ì˜¤ë¥˜ êµ¬ê°„ ëë‚¬ëŠ”ì§€ ì²´í¬
            if cur_err_end is not None and idx >= cur_err_end:
                result_parts.append("</mark>")
                in_error = False
                interval_idx += 1
                cur_err_end = None
        else:
            # ì˜¤ë¥˜ êµ¬ê°„ ë°–: ë¬¸ì¥ë¶€í˜¸ë©´ ìƒ‰ìƒ í•˜ì´ë¼ì´íŠ¸
            if ch in PUNCT_CHARS and (selected_punct_chars is None or ch in selected_punct_chars):
                color = PUNCT_COLOR_MAP.get(ch, "#e2e3e5")
                result_parts.append(
                    f"<span style='background-color: {color}; padding: 0 2px; font-weight: 700; font-size: 1.05em; border-radius: 2px;'>{html.escape(ch)}</span>"
                )
            else:
                result_parts.append(html.escape(ch))
            idx += 1

    # í˜¹ì‹œ ì˜¤ë¥˜ êµ¬ê°„ì´ ì—´ë¦° ì±„ë¡œ ëë‚œ ê²½ìš° ë‹«ì•„ì£¼ê¸° (ì´ë¡ ìƒ ê±°ì˜ ì—†ìŒ)
    if in_error:
        result_parts.append("</mark>")

    return "".join(result_parts)


def highlight_selected_punctuation(source_text: str, selected_keys: list[str]) -> str:
    """
    ì„ íƒëœ ë¬¸ì¥ë¶€í˜¸ ê·¸ë£¹ë§Œ ìƒ‰ìƒ í•˜ì´ë¼ì´íŠ¸í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ë³´ì—¬ì¤€ë‹¤.
    """
    if not source_text:
        return ""

    selected_chars: set[str] = set()
    for key in selected_keys:
        selected_chars.update(PUNCT_GROUPS.get(key, set()))

    result_parts: List[str] = []
    for ch in source_text:
        if ch in selected_chars and ch in PUNCT_COLOR_MAP:
            color = PUNCT_COLOR_MAP.get(ch, "#e2e3e5")
            result_parts.append(
                f"<span style='background-color: {color}; padding: 0 3px; font-weight: 700; font-size: 1.1em; border-radius: 3px;'>{html.escape(ch)}</span>"
            )
        else:
            result_parts.append(html.escape(ch))
    return "".join(result_parts)




def analyze_text_with_gemini(prompt: str, max_retries: int = 5) -> dict:
    """
    ë‹¨ì¼ í…ìŠ¤íŠ¸ ê²€ì‚¬ìš© Gemini í˜¸ì¶œ.
    í•­ìƒ dictë¥¼ ë¦¬í„´í•˜ë„ë¡ ë°©ì–´ ë¡œì§ì„ ë„£ìŒ.
    """
    last_error: Exception | None = None

    for attempt in range(max_retries):
        try:
            generation_config = {
                "response_mime_type": "application/json",
                "temperature": 0.0,
            }
            response = model.generate_content(
                prompt,
                generation_config=generation_config,
            )

            raw = getattr(response, "text", None)
            if raw is None or not str(raw).strip():
                return {
                    "suspicion_score": 5,
                    "content_typo_report": "AI ì‘ë‹µì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.",
                    "translated_typo_report": "",
                    "markdown_report": "",
                }

            obj = json.loads(raw)

            if not isinstance(obj, dict):
                return {
                    "suspicion_score": 5,
                    "content_typo_report": f"AI ì‘ë‹µì´ dictê°€ ì•„ë‹˜ (type={type(obj).__name__})",
                    "translated_typo_report": "",
                    "markdown_report": "",
                }

            return obj

        except Exception as e:
            last_error = e
            wait_time = 5 * (attempt + 1)
            print(f"[Gemini(single)] í˜¸ì¶œ ì˜¤ë¥˜ (ì‹œë„ {attempt+1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                print(f"â†’ {wait_time}ì´ˆ í›„ ì¬ì‹œë„")
                time.sleep(wait_time)

    print("[Gemini(single)] ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼.")
    return {
        "suspicion_score": 5,
        "content_typo_report": f"API í˜¸ì¶œ ì‹¤íŒ¨: {last_error}",
        "translated_typo_report": "",
        "markdown_report": "",
    }


def drop_lines_not_in_source(source_text: str, report: str) -> str:
    """
    '- 'ì›ë¬¸' â†’ 'ìˆ˜ì •ì•ˆ': ...' í˜•ì‹ì—ì„œ
    'ì›ë¬¸'ì´ ì‹¤ì œ source_textì— í¬í•¨ë˜ì§€ ì•Šì€ ë¼ì¸ì„ ì œê±°.
    (í•œêµ­ì–´/ì˜ì–´ ê³µí†µ ì‚¬ìš©)
    """
    if not report:
        return ""

    cleaned: List[str] = []
    pattern = re.compile(r"^- '(.+?)' â†’ '(.+?)':", re.UNICODE)
    
    pattern = re.compile(
        r"""^-\s*(['"])(.+?)\1\s*(?:â†’|->)\s*(['"])(.+?)\3\s*:\s*(.+)$""",
        re.UNICODE,
    )

    for line in report.splitlines():
        s = line.strip()
        if not s:
            continue

        m = pattern.match(s)
        if not m:
            cleaned.append(s)
            continue

        original = m.group(2)
        if original in source_text:
            cleaned.append(s)
        else:
            continue

    return "\n".join(cleaned)


def clean_self_equal_corrections(report: str) -> str:
    """
    '- 'ì›ë¬¸' â†’ 'ìˆ˜ì •ì•ˆ': ...' í˜•ì‹ì—ì„œ
    ì›ë¬¸ê³¼ ìˆ˜ì •ì•ˆì´ ì™„ì „íˆ ê°™ì€ ì¤„ì€ ì œê±°í•œë‹¤.
    (ì£¼ë¡œ ì˜ì–´ ìª½ content_typo_reportì— ì‚¬ìš©)
    """
    
    pattern = re.compile(
    r"""^-\s*(['"])(.+?)\1\s*(?:â†’|->)\s*(['"])(.+?)\3\s*:""",
    re.UNICODE,
)

    if not report:
        return ""

    cleaned_lines = []
    pattern = re.compile(r"^- '(.+?)' â†’ '(.+?)':", re.UNICODE)

    for line in report.splitlines():
        line_stripped = line.strip()
        if not line_stripped:
            continue

        m = pattern.match(line_stripped)
        if not m:
            cleaned_lines.append(line_stripped)
            continue

        orig = m.group(1).strip()
        fixed = m.group(2).strip()

        if orig == fixed:
            continue

        cleaned_lines.append(line_stripped)

    return "\n".join(cleaned_lines)


def drop_false_period_errors(english_text: str, report: str) -> str:
    """
    ì˜ì–´ ì›ë¬¸ ëì— ì‹¤ì œë¡œ . ? ! ì´ ìˆìœ¼ë©´
    ë¦¬í¬íŠ¸ì—ì„œ 'ë§ˆì¹¨í‘œ ì—†ìŒ'ë¥˜ ë¬¸ì¥ì„ ì œê±°.
    (ê±°ì§“ ì–‘ì„± ì¤„ì´ê¸°ìš©)
    """
    
    pattern = re.compile(
    r"""^-\s*(['"])(.+?)\1\s*(?:â†’|->)\s*(['"])(.+?)\3\s*:""",
    re.UNICODE,
)

    if not report:
        return ""

    stripped = (english_text or "").rstrip()
    last_char = stripped[-1] if stripped else ""

    if last_char in [".", "?", "!"]:
        bad_phrases = [
            "ë§ˆì¹¨í‘œê°€ ì—†ìŠµë‹ˆë‹¤",
            "ë§ˆì¹¨í‘œê°€ ë¹ ì ¸",
            "ë§ˆì¹¨í‘œê°€ í•„ìš”",
            "ë§ˆì¹¨í‘œë¥¼ ì°ì–´ì•¼",
        ]
        cleaned_lines = []
        for line in report.splitlines():
            if any(p in line for p in bad_phrases):
                continue
            cleaned_lines.append(line.strip())
        return "\n".join(cleaned_lines)

    return report


def drop_false_korean_period_errors(report: str) -> str:
    """
    í•œêµ­ì–´ ë¦¬í¬íŠ¸ì—ì„œ, 'ì›ë¬¸' ë¶€ë¶„ì— ì´ë¯¸ ì¢…ê²°ë¶€í˜¸ê°€ ìˆëŠ”ë°
    'ë§ˆì¹¨í‘œê°€ ì—†ìŠµë‹ˆë‹¤' ë¥˜ë¡œ ì˜ëª» ë³´ê³ í•œ ì¤„ì„ ì œê±°í•œë‹¤.
    """
    if not report:
        return ""

    cleaned_lines = []
    pattern = re.compile(r"^- '(.+?)' â†’ '(.+?)':", re.UNICODE)
    bad_phrases = [
        "ë§ˆì¹¨í‘œê°€ ì—†ìŠµë‹ˆë‹¤",
        "ë§ˆì¹¨í‘œê°€ ë¹ ì ¸",
        "ë§ˆì¹¨í‘œê°€ í•„ìš”",
        "ë§ˆì¹¨í‘œë¥¼ ì°ì–´ì•¼",
        "ë¬¸ì¥ ëì— ë§ˆì¹¨í‘œê°€ ì—†",
    ]

    for line in report.splitlines():
        s = line.strip()
        if not s:
            continue

        if not any(p in s for p in bad_phrases):
            cleaned_lines.append(s)
            continue

        m = pattern.match(s)
        if not m:
            cleaned_lines.append(s)
            continue

        original = m.group(1).rstrip()
        if not original:
            cleaned_lines.append(s)
            continue

        last = original[-1]
        ok = False
        if last in ".?!":
            ok = True
        elif len(original) >= 2 and last in ['"', "'", "â€", "â€™", "ã€", "ã€", "ã€‹", "ã€‰", ")", "]"] and original[-2] in ".?!":
            ok = True

        if ok:
            # ì´ë¯¸ ì¢…ê²°ë¶€í˜¸ê°€ ìˆëŠ” ë¬¸ì¥ì¸ë° 'ë§ˆì¹¨í‘œ ì—†ìŒ'ì´ë¼ê³  í•œ ì¤„ â†’ ë²„ë¦¼
            continue
        else:
            cleaned_lines.append(s)

    return "\n".join(cleaned_lines)


def drop_false_whitespace_claims(text: str, report: str) -> str:
    """
    'ë¶ˆí•„ìš”í•œ ê³µë°±'ë¥˜ë¥¼ ì§€ì í–ˆì§€ë§Œ ì›ë¬¸ ì¡°ê°ì— ê³µë°±/ì œë¡œí­ ê³µë°±ì´ ì „í˜€ ì—†ìœ¼ë©´ ì œê±°í•œë‹¤.
    """
    if not report:
        return ""

    cleaned: list[str] = []
    pattern = re.compile(r"^- '(.+?)' â†’ '(.+?)':.*(ë¶ˆí•„ìš”í•œ ê³µë°±|ë„ì–´ì“°ê¸°|ê³µë°±)", re.UNICODE)

    for line in report.splitlines():
        s = line.strip()
        if not s:
            continue

        m = pattern.match(s)
        if not m:
            cleaned.append(s)
            continue

        original = m.group(1)
        # ì‹¤ì œ ê³µë°±/ì œë¡œí­ ê³µë°±ì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ì˜¤íƒìœ¼ë¡œ ê°„ì£¼
        if not re.search(r"[ \t\u3000\u200b\u200c\u200d]", original):
            continue

        cleaned.append(s)

    return "\n".join(cleaned)


def ensure_final_punctuation_error(text: str, report: str) -> str:
    if not text or not text.strip():
        return report or ""

    s = text.rstrip()
    if not s:
        return report or ""

    last = s[-1]

    end_ok = False
    if last in ".?!":
        end_ok = True
    elif last in ['"', "'", "â€", "â€™", "ã€", "ã€", "ã€‹", "ã€‰", ")", "]"] and len(s) >= 2 and s[-2] in ".?!":
        end_ok = True

    if end_ok:
        return report or ""

    # ì´ë¯¸ ë¹„ìŠ·í•œ ë‚´ìš©ì´ ìˆìœ¼ë©´ ì¤‘ë³µìœ¼ë¡œ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
    if report and ("ë§ˆì¹¨í‘œ" in report or "ë¬¸ì¥ë¶€í˜¸" in report):
        return report

    # ğŸ”´ ì—¬ê¸°ì—ì„œ 'ìˆ˜ ìˆì—ˆë‹¤' ê°™ì€ ì˜ˆì‹œë¥¼ ì“°ì§€ ë§ê³ ,
    #     ê·¸ëƒ¥ ì„¤ëª…ë§Œ ì¶”ê°€í•œë‹¤.
    line = "- ë¬¸ë‹¨ ë§ˆì§€ë§‰ ë¬¸ì¥ ëì— ë§ˆì¹¨í‘œ(ë˜ëŠ” ë¬¼ìŒí‘œ, ëŠë‚Œí‘œ)ê°€ ë¹ ì ¸ ìˆìœ¼ë¯€ë¡œ ì ì ˆí•œ ë¬¸ì¥ë¶€í˜¸ë¥¼ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤."

    if report:
        return report.rstrip() + "\n" + line
    else:
        return line



def ensure_english_final_punctuation(text: str, report: str) -> str:
    """
    ì˜ì–´ í…ìŠ¤íŠ¸ì˜ 'ë§ˆì§€ë§‰ ë¬¸ì¥'ì´ ., ?, ! ë¡œ ëë‚˜ì§€ ì•Šìœ¼ë©´
    ì•„ì£¼ ë³´ìˆ˜ì ì¸ ìš”ì•½ ê²½ê³  í•œ ì¤„ì„ ì¶”ê°€í•œë‹¤.
    (ì‰¼í‘œ/ì„¸ë¯¸ì½œë¡ /ì½œë¡  ë“±ìœ¼ë¡œ ëë‚˜ëŠ” ê²½ìš° í¬í•¨)
    """
    if not text or not text.strip():
        return report or ""

    s = text.rstrip()
    if not s:
        return report or ""

    last = s[-1]

    end_ok = False
    if last in ".?!":
        end_ok = True
    # ë”°ì˜´í‘œ/ê´„í˜¸ ë’¤ì— .?! ê°€ ìˆëŠ” ê²½ìš° í—ˆìš©
    elif last in ['"', "'", ")", "]", "â€", "â€™"] and len(s) >= 2 and s[-2] in ".?!":
        end_ok = True

    if end_ok:
        return report or ""

    # ì´ë¯¸ ë¹„ìŠ·í•œ ë¬¸êµ¬ê°€ ìˆìœ¼ë©´ ì¤‘ë³µ ì¶”ê°€ ë°©ì§€
    if report and ("ì¢…ê²°ë¶€í˜¸" in report or "ë§ˆì¹¨í‘œ" in report or "punctuation" in report):
        return report

    line = "- ë§ˆì§€ë§‰ ë¬¸ì¥ì´ ì¢…ê²°ë¶€í˜¸(., ?, !)ê°€ ì•„ë‹Œ ë¬¸ì¥ë¶€í˜¸ë¡œ ëë‚˜ ìˆì–´, ë¬¸ì¥ì„ ë§ˆì¹¨í‘œ ë“±ìœ¼ë¡œ ëª…í™•íˆ ëë‚´ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤."

    if report:
        return report.rstrip() + "\n" + line
    else:
        return line



def ensure_sentence_end_punctuation(text: str, report: str) -> str:
    """
    ë¬¸ë‹¨ ë‚´ ëª¨ë“  ë¬¸ì¥ì˜ ëì— ì¢…ê²°ë¶€í˜¸(. ? !)ê°€ ìˆëŠ”ì§€ ëŒ€ëµ ê²€ì‚¬.
    ëˆ„ë½ëœ ë¬¸ì¥ì´ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ ìš”ì•½ ë©”ì‹œì§€ë¥¼ ì¶”ê°€.
    ë‹¤ë§Œ ì´ë¯¸ ë‹¤ë¥¸ ì¤„ì—ì„œ ì¢…ê²°ë¶€í˜¸ ëˆ„ë½ì„ êµ¬ì²´ì ìœ¼ë¡œ ì–¸ê¸‰í–ˆë‹¤ë©´
    ì¤‘ë³µ ë©”ì‹œì§€ëŠ” ì¶”ê°€í•˜ì§€ ì•ŠëŠ”ë‹¤.
    """
    if not text or not text.strip():
        return report or ""

    sentences = re.split(r'(?<=[.!?])\s+', text.strip())
    missing = []

    for s in sentences:
        s = s.strip()
        if not s:
            continue

        ok = False
        if s[-1] in ".?!":
            ok = True
        elif len(s) >= 2 and s[-1] in ['"', "'", "â€", "â€™", "ã€", "ã€", "ã€‹", "ã€‰", ")", "]"] and s[-2] in ".?!":
            ok = True

        if not ok:
            missing.append(s)

    if not missing:
        return report or ""

    # ì´ë¯¸ ì¢…ê²°ë¶€í˜¸ ê´€ë ¨ ë©˜íŠ¸ê°€ ìˆìœ¼ë©´ ìš”ì•½ ì¤„ ìƒëµ
    if report and any(
        key in report
        for key in ["ë§ˆì§€ë§‰ ë¬¸ì¥ì— ë§ˆì¹¨í‘œ", "ì¢…ê²°ë¶€í˜¸", "ë¬¸ì¥ ëì— ë§ˆì¹¨í‘œê°€ ì—†", "ë§ˆì¹¨í‘œê°€ ì—†ìŠµë‹ˆë‹¤"]
    ):
        return report

    line = "- ë¬¸ì¥ ëì— ì¢…ê²°ë¶€í˜¸(., ?, !)ê°€ ëˆ„ë½ëœ ë¬¸ì¥ì´ ìˆìŠµë‹ˆë‹¤."

    if report:
        return report.rstrip() + "\n" + line
    else:
        return line


def dedup_korean_bullet_lines(report: str) -> str:
    """
    í•œêµ­ì–´ bullet ë¦¬í¬íŠ¸ì—ì„œ ì˜ë¯¸ê°€ ê²¹ì¹˜ëŠ” ì¤„ì„ ì •ë¦¬í•œë‹¤.
    - ì™„ì „íˆ ë™ì¼í•œ ì¤„ì€ í•˜ë‚˜ë§Œ ë‚¨ê¹€
    - 'ë¶ˆí•„ìš”í•œ ë§ˆì¹¨í‘œ'ë¥˜ì—ì„œ ì›ë¬¸ì´ ë¶€ë¶„ ë¬¸ìì—´ ê´€ê³„ì´ë©´ ë” ê¸´ ìª½ë§Œ ìœ ì§€
    """
    
    pattern = re.compile(
    r"""^-\s*(['"])(.+?)\1\s*(?:â†’|->)\s*(['"])(.+?)\3\s*:""",
    re.UNICODE,
    )

    if not report:
        return ""

    lines = [l.strip() for l in report.splitlines() if l.strip()]
    if not lines:
        return ""

    pattern = re.compile(r"^- '(.+?)' â†’ '(.+?)':\s*(.+)$", re.UNICODE)

    # 1ì°¨: ì™„ì „ ì¤‘ë³µ ì œê±°
    unique_lines = []
    seen = set()
    for l in lines:
        if l not in seen:
            unique_lines.append(l)
            seen.add(l)

    entries = []
    for idx, l in enumerate(unique_lines):
        m = pattern.match(l)
        if not m:
            entries.append({"idx": idx, "raw": l, "orig": None, "msg": ""})
            continue
        orig, fixed, msg = m.group(1), m.group(2), m.group(3)
        entries.append({"idx": idx, "raw": l, "orig": orig, "msg": msg})

    to_drop = set()
    for i, e1 in enumerate(entries):
        if not e1["orig"] or "ë¶ˆí•„ìš”í•œ ë§ˆì¹¨í‘œ" not in e1["msg"]:
            continue
        for j, e2 in enumerate(entries):
            if i == j or not e2["orig"] or "ë¶ˆí•„ìš”í•œ ë§ˆì¹¨í‘œ" not in e2["msg"]:
                continue
            o1, o2 = e1["orig"], e2["orig"]
            if o1 in o2 and len(o1) < len(o2):
                to_drop.add(e1["idx"])
            elif o2 in o1 and len(o2) < len(o1):
                to_drop.add(e2["idx"])

    final_lines = [
        l for idx, l in enumerate(unique_lines) if idx not in to_drop
    ]

    return "\n".join(final_lines)


def validate_and_clean_analysis(result: dict, original_english_text: str | None = None) -> dict:
    """
    AI ì‘ë‹µì—ì„œ ë¬¸ì²´ ì œì•ˆ ë“±ì„ í•„í„°ë§í•˜ê³  ì ìˆ˜ë¥¼ ë³´ì • + (ì˜ì–´ ìª½ ì¶”ê°€ í›„ì²˜ë¦¬)
    """
    if not isinstance(result, dict):
        return {
            "suspicion_score": 5,
            "content_typo_report": "AI ì‘ë‹µì´ ìœ íš¨í•œ JSON í˜•ì‹ì´ ì•„ë‹˜",
            "translated_typo_report": "",
            "markdown_report": "",
        }

    score = result.get("suspicion_score")
    reports = {
        "content_typo_report": result.get("content_typo_report", "") or "",
        "translated_typo_report": result.get("translated_typo_report", "") or "",
        "markdown_report": result.get("markdown_report", "") or "",
    }

    # ìŠ¤íƒ€ì¼/ë¬¸ì²´ ì œì•ˆ ê¸ˆì§€ í‚¤ì›Œë“œ í•„í„°
    forbidden_keywords = [
        "ë¬¸ë§¥ìƒ",
        "ë¶€ì ì ˆ",
        "ì–´ìƒ‰",
        "ë” ìì—°ìŠ¤ëŸ½",
        "ë” ì ì ˆ",
        "ìˆ˜ì •í•˜ëŠ” ê²ƒì´ ì¢‹",
        "ì œì•ˆ",
        "ë°”ê¾¸ëŠ” ê²ƒ",
        "ì˜ë¯¸ë¥¼ ëª…í™•íˆ",
    ]
    for key, text in reports.items():
        if any(kw in text for kw in forbidden_keywords):
            reports[key] = ""

    # "ì˜¤ë¥˜ ì—†ìŒ"ë¥˜ ë©˜íŠ¸ ì œê±°
    forbidden_phrases = ["ì˜¤ë¥˜ ì—†ìŒ", "ì •ìƒ", "ë¬¸ì œ ì—†ìŒ", "ìˆ˜ì •í•  í•„ìš” ì—†ìŒ"]
    for key, text in reports.items():
        if any(ph in text for ph in forbidden_phrases):
            reports[key] = ""

    # ì˜ì–´ ë¦¬í¬íŠ¸ í›„ì²˜ë¦¬
    english_report = reports["content_typo_report"]
    english_report = clean_self_equal_corrections(english_report)
    if original_english_text:
        english_report = drop_false_period_errors(original_english_text, english_report)
    reports["content_typo_report"] = english_report

    final_content = reports["content_typo_report"]
    final_translated = reports["translated_typo_report"]
    final_markdown = reports["markdown_report"]

    # score ê¸°ë³¸ê°’ ë³´ì •
    try:
        score = int(score)
    except Exception:
        score = 1

    if score < 1:
        score = 1
    if score > 5:
        score = 5

    if not final_content and not final_translated and not final_markdown:
        score = 1
    elif (final_content or final_translated or final_markdown) and score == 1:
        score = 3

    return {
        "suspicion_score": score,
        "content_typo_report": final_content,
        "translated_typo_report": final_translated,
        "markdown_report": final_markdown,
    }


# -------------------------------------------------
# 1-A. í•œêµ­ì–´ ë‹¨ì¼ í…ìŠ¤íŠ¸ ê²€ìˆ˜ í”„ë¡¬í”„íŠ¸ + ë˜í¼
# -------------------------------------------------

def create_korean_detector_prompt_for_text(korean_text: str) -> str:
    """
    1ì°¨ íŒ¨ìŠ¤: Detector
    - ê°€ëŠ¥í•œ ë§ì€ 'ì ì¬ì  ì˜¤ë¥˜ í›„ë³´'ë¥¼ ì°¾ëŠ” ì—­í•  (ì•½ê°„ ê³¼ê²€ì¶œ í—ˆìš©)
    """
    safe_text = json.dumps(korean_text, ensure_ascii=False)

    prompt = f"""
ë‹¹ì‹ ì€ 1ì°¨ **Korean text proofreader (Detector)**ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì•„ë˜ í•œêµ­ì–´ í…ìŠ¤íŠ¸ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ”
**ëª¨ë“  ì ì¬ì  ì˜¤ë¥˜ í›„ë³´ë¥¼ ìµœëŒ€í•œ ë§ì´ íƒì§€í•˜ëŠ” ê²ƒ**ì…ë‹ˆë‹¤.

ì´ ë‹¨ê³„ì—ì„œëŠ” ì•½ê°„ì˜ ê³¼ì‰ íƒì§€(False Positive)ë¥¼ í—ˆìš©í•©ë‹ˆë‹¤.
(2ì°¨ Judge ë‹¨ê³„ì—ì„œ ì˜ë¯¸ ë³€ê²½Â·ìŠ¤íƒ€ì¼ ì œì•ˆ ë“±ì€ ì œê±°ë©ë‹ˆë‹¤.)

ì¶œë ¥ì€ ë°˜ë“œì‹œ ì•„ë˜ 4ê°œì˜ keyë§Œ í¬í•¨í•˜ëŠ” **ë‹¨ì¼ JSON ê°ì²´**ì—¬ì•¼ í•©ë‹ˆë‹¤.
- "suspicion_score": 1~5 ì •ìˆ˜
- "content_typo_report": "" (ë¹„ì›Œë‘ê¸° â€” ì˜ì–´ìš© í•„ë“œ)
- "translated_typo_report": "- 'ì›ë¬¸' â†’ 'ìˆ˜ì •ì•ˆ': ì„¤ëª…" í˜•ì‹ì˜ ì¤„ì„ ì—¬ëŸ¬ ê°œ í¬í•¨í•œ ë¬¸ìì—´ (ì—†ìœ¼ë©´ "")
- "markdown_report": "" (í•­ìƒ ë¹ˆ ë¬¸ìì—´)

ëª¨ë“  ì„¤ëª…ì€ ë°˜ë“œì‹œ **í•œêµ­ì–´ë¡œ** ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.

------------------------------------------------------------
# ì…ë ¥ í…ìŠ¤íŠ¸ (JSON ë¬¸ìì—´)
------------------------------------------------------------

ì•„ë˜ëŠ” ì „ì²´ í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ JSON ë¬¸ìì—´ë¡œ ì¸ì½”ë”©í•œ ê°’ì…ë‹ˆë‹¤.
ì´ ê°’ì„ ê·¸ëŒ€ë¡œ ë””ì½”ë”©í•œ í…ìŠ¤íŠ¸(plain_korean)ë¥¼ ê¸°ì¤€ìœ¼ë¡œë§Œ ê²€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤.

plain_korean_json: {safe_text}

- plain_korean_jsonì„ ë””ì½”ë”©í•œ ê²°ê³¼ë¥¼ plain_koreanì´ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤.
- "- 'ì›ë¬¸' â†’ 'ìˆ˜ì •ì•ˆ': ì„¤ëª…" í˜•ì‹ì—ì„œ 'ì›ë¬¸'ì€
  ë°˜ë“œì‹œ plain_korean ì•ˆì— ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ë¶€ë¶„ ë¬¸ìì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤.

------------------------------------------------------------
# 1. ì´ ë‹¨ê³„ì—ì„œ ê¼­ ì¡ì•„ì•¼ í•˜ëŠ” ì˜¤ë¥˜ (ë„“ê²Œ íƒì§€)
------------------------------------------------------------

- ëª…ë°±í•œ ì˜¤íƒˆì, ì² ì ì˜¤ë¥˜
- ì˜ëª»ëœ ë„ì–´ì“°ê¸°/ë¶™ì—¬ì“°ê¸°
- ì¡°ì‚¬Â·ì–´ë¯¸ ì˜¤ìš©
- ë¬¸ì¥ë¶€í˜¸ ì˜¤ë¥˜ (ë§ˆì¹¨í‘œ/ì‰¼í‘œ/ë”°ì˜´í‘œ ì§/ê´„í˜¸ ì§ ë“±)
- ë‹¨ì–´ ë‚´ë¶€ê°€ ì´ìƒí•˜ê²Œ ë¶„ë¦¬ëœ ê²½ìš° (ì˜ˆ: "ëœ ë‹¤", "í•˜ ì˜€ë‹¤" ë“±)

ì´ ë‹¨ê³„ì—ì„œëŠ” ë‹¤ì†Œ ì• ë§¤í•œ ê²ƒê¹Œì§€ **í›„ë³´ë¡œ ì¡ì•„ë„** ê´œì°®ìŠµë‹ˆë‹¤.
2ì°¨ Judgeê°€ ì˜ë¯¸ ë³€ê²½/ìŠ¤íƒ€ì¼ ì œì•ˆ ë“±ì„ í•„í„°ë§í•©ë‹ˆë‹¤.

ì´ì œ plain_korean_jsonì„ ë””ì½”ë”©í•˜ì—¬ plain_koreanì„ ì–»ì€ ë’¤,
ìœ„ ê¸°ì¤€ì— ë”°ë¼ "- 'ì›ë¬¸' â†’ 'ìˆ˜ì •ì•ˆ': ì„¤ëª…" í˜•ì‹ìœ¼ë¡œ translated_typo_reportë¥¼ ìƒì„±í•˜ì‹­ì‹œì˜¤.
"""
    return prompt


def create_korean_judge_prompt_for_text(korean_text: str, draft_report: str) -> str:
    """
    2ì°¨ íŒ¨ìŠ¤: Judge
    - 1ì°¨ Detectorê°€ ë§Œë“  í›„ë³´ë“¤(draft_report) ì¤‘ì—ì„œ
      'ì˜ë¯¸ë¥¼ ë°”ê¾¸ì§€ ì•ŠëŠ” ê°ê´€ì ì¸ ì˜¤ë¥˜ ìˆ˜ì •'ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ë¥¼ ì œê±°í•˜ëŠ” ì—­í• .
    """
    safe_text = json.dumps(korean_text, ensure_ascii=False)
    safe_report = json.dumps(draft_report, ensure_ascii=False)

    prompt = f"""
ë‹¹ì‹ ì€ 2ì°¨ **Korean text proofreader (Judge)**ì…ë‹ˆë‹¤.

ì—­í• :
- 1ì°¨ Detectorê°€ ë§Œë“  ì˜¤ë¥˜ í›„ë³´ ëª©ë¡(draft_report) ì¤‘ì—ì„œ
  **ì˜ë¯¸ë¥¼ ë°”ê¾¸ì§€ ì•ŠëŠ” ê°ê´€ì ì¸ ì˜¤ë¥˜ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” ëª¨ë‘ ì œê±°**í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

------------------------------------------------------------
# ì…ë ¥ 1: ì „ì²´ í•œêµ­ì–´ ì›ë¬¸ (JSON ë¬¸ìì—´)
------------------------------------------------------------
plain_korean_json: {safe_text}

- plain_korean_jsonì„ ë””ì½”ë”©í•œ ê²°ê³¼ë¥¼ plain_koreanì´ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤.

------------------------------------------------------------
# ì…ë ¥ 2: 1ì°¨ Detectorì˜ í›„ë³´ ë¦¬í¬íŠ¸ (JSON ë¬¸ìì—´)
------------------------------------------------------------
draft_report_json: {safe_report}

- draft_report_jsonì€ ë¬¸ìì—´ì´ë©°,
  ë‚´ë¶€ í˜•ì‹ì€ "- 'ì›ë¬¸' â†’ 'ìˆ˜ì •ì•ˆ': ì„¤ëª…" ì¤„ë“¤ì´ ì¤„ë°”ê¿ˆìœ¼ë¡œ ì´ì–´ì§„ í˜•íƒœì…ë‹ˆë‹¤.

ê° ì¤„ì— ëŒ€í•´ ì•„ë˜ ê¸°ì¤€ìœ¼ë¡œ **ì±„íƒ/ì œê±° ì—¬ë¶€**ë¥¼ íŒë‹¨í•˜ì‹­ì‹œì˜¤.

------------------------------------------------------------
# ì±„íƒ ê¸°ì¤€ (ëª¨ë“  ì¡°ê±´ì„ ë§Œì¡±í•´ì•¼ í•¨)
------------------------------------------------------------

1. 'ì›ë¬¸'ì€ plain_korean ì•ˆì— ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ë¶€ë¶„ ë¬¸ìì—´ì´ì–´ì•¼ í•œë‹¤.
2. 'ìˆ˜ì •ì•ˆ'ì€ ë‹¤ìŒê³¼ ê°™ì€ **í˜•ì‹ì Â·ê°ê´€ì  ìˆ˜ì •**ë§Œ í¬í•¨í•´ì•¼ í•œë‹¤.
   - ë„ì–´ì“°ê¸°/ë¶™ì—¬ì“°ê¸° ìˆ˜ì •
   - ì¡°ì‚¬/ì–´ë¯¸ êµì •
   - ëª…ë°±í•œ ì˜¤íƒˆìÂ·ì² ì ì˜¤ë¥˜
   - ë¬¸ì¥ë¶€í˜¸(ë§ˆì¹¨í‘œ, ì‰¼í‘œ, ë”°ì˜´í‘œ, ê´„í˜¸ ë“±) êµì •
3. ì˜ë¯¸ë¥¼ ë°”ê¾¸ëŠ” ì–´íœ˜ ë³€ê²½ì´ë‚˜ ë¬¸ì¥ êµ¬ì¡° ë³€ê²½ì€ ëª¨ë‘ ì œê±°í•œë‹¤.
4. ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„, ë¬¸ì²´ ê°œì„ , í†¤ ì¡°ì •, ê¸¸ì´ ì¤„ì´ê¸°/ëŠ˜ë¦¬ê¸° ë“±
   **ìŠ¤íƒ€ì¼/í‘œí˜„ ê°œì„  ëª©ì ì˜ ìˆ˜ì •**ì€ ëª¨ë‘ ì œê±°í•œë‹¤.
5. plain_koreanì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´Â·êµ¬ì ˆì„ 'ì›ë¬¸'ìœ¼ë¡œ ì¸ìš©í•œ ì¤„ì€ ì œê±°í•œë‹¤.

------------------------------------------------------------
# ì¶œë ¥
------------------------------------------------------------

ë°˜í™˜ ê°’ì€ ë°˜ë“œì‹œ ì•„ë˜ 4ê°œì˜ keyë¥¼ ê°€ì§„ **ë‹¨ì¼ JSON ê°ì²´**ì—¬ì•¼ í•©ë‹ˆë‹¤.
- "suspicion_score": 1~5 ì •ìˆ˜ (ë‚¨ì€ ì˜¤ë¥˜ í›„ë³´ì˜ ì‹¬ê°ë„ì— ë”°ë¼ íŒë‹¨)
- "content_typo_report": "" (ë¹„ì›Œë‘ê¸°)
- "translated_typo_report":
    draft_report_jsonì— í¬í•¨ëœ ì¤„ë“¤ ì¤‘ì—ì„œ
    ìœ„ ê¸°ì¤€ì„ ë§Œì¡±í•˜ëŠ” ì¤„ë§Œ ë‚¨ê¸´ "- 'ì›ë¬¸' â†’ 'ìˆ˜ì •ì•ˆ': ì„¤ëª…" ë¬¸ìì—´
    (ê° ì¤„ì€ ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„)
- "markdown_report": "" (í•­ìƒ ë¹ˆ ë¬¸ìì—´)

draft_report_jsonì— ìˆë˜ ì¤„ì´ë¼ë„, ìœ„ ê¸°ì¤€ì„ ë§Œì¡±í•˜ì§€ ëª»í•˜ë©´
í•´ë‹¹ ì¤„ì€ ì™„ì „íˆ ì œê±°í•˜ì—¬ translated_typo_reportì— í¬í•¨í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
"""
    return prompt

# -------- Stage helpers (Detector / Judge / Final) --------

def get_korean_stage_reports(raw_bundle: dict, final_report: str) -> dict:
    """
    í•œêµ­ì–´ 1ì°¨ / 2ì°¨ / ìµœì¢… ë¦¬í¬íŠ¸ ë¬¸ìì—´ì„ stageë³„ë¡œ ëŒë ¤ì¤€ë‹¤.
    return ì˜ˆì‹œ:
    {
        "detector": "...",
        "judge": "...",
        "final": "..."
    }
    """
    if not isinstance(raw_bundle, dict):
        raw_bundle = {}

    detector_report = ""
    judge_report = ""

    # chunked ëª¨ë“œ: ë¸”ë¡ë³„ ë¦¬í¬íŠ¸ë¥¼ í—¤ë”ì™€ í•¨ê»˜ ì´ì–´ë¶™ì¸ë‹¤.
    if raw_bundle.get("mode") == "chunked":
        det_lines: list[str] = []
        judge_lines: list[str] = []
        for chunk in raw_bundle.get("chunks", []):
            idx = chunk.get("index")
            raw = chunk.get("raw") or {}

            det_line = ""
            det_line = (raw.get("initial_report_from_detector") or "").strip()
            if not det_line:
                det_clean = raw.get("detector_clean") or {}
                if isinstance(det_clean, dict):
                    det_line = (det_clean.get("translated_typo_report") or "").strip()

            judge_line = (raw.get("final_report_before_rule_postprocess") or "").strip()
            if not judge_line:
                judge_clean = raw.get("judge_clean") or {}
                if isinstance(judge_clean, dict):
                    judge_line = (judge_clean.get("translated_typo_report") or "").strip()
            if not judge_line:
                judge_line = (raw.get("translated_typo_report") or "").strip()

            header = f"# [ë¸”ë¡ {idx}]" if idx is not None else None
            if det_line:
                if header:
                    det_lines.append(header)
                det_lines.append(det_line)
            if judge_line:
                if header:
                    judge_lines.append(header)
                judge_lines.append(judge_line)

        detector_report = "\n".join(det_lines).strip()
        judge_report = "\n".join(judge_lines).strip()

    else:
        # ë‹¨ì¼ ë¸”ë¡ ëª¨ë“œ
        detector_clean = raw_bundle.get("detector_clean") or {}
        if isinstance(detector_clean, dict):
            detector_report = (detector_clean.get("translated_typo_report") or "").strip()

        judge_clean = raw_bundle.get("judge_clean") or {}
        if isinstance(judge_clean, dict):
            judge_report = (judge_clean.get("translated_typo_report") or "").strip()
        if not judge_report:
            judge_report = (raw_bundle.get("translated_typo_report") or "").strip()

    return {
        "detector": detector_report,
        "judge": judge_report,
        "final": (final_report or "").strip(),
    }


def get_english_stage_reports(raw_bundle: dict, final_report: str) -> dict:
    """
    ì˜ì–´ 1ì°¨ / 2ì°¨ / ìµœì¢… ë¦¬í¬íŠ¸ ë°˜í™˜
    """
    if not isinstance(raw_bundle, dict):
        raw_bundle = {}

    # 1ì°¨ Detector: initial_report_from_detector ìš°ì„ 
    detector_report = (raw_bundle.get("initial_report_from_detector") or "").strip()
    if not detector_report:
        detector_clean = raw_bundle.get("detector_clean") or {}
        if isinstance(detector_clean, dict):
            detector_report = (detector_clean.get("content_typo_report") or "").strip()

    # 2ì°¨ Judge: final_report_before_rule_postprocess ìš°ì„ 
    judge_report = (raw_bundle.get("final_report_before_rule_postprocess") or "").strip()
    if not judge_report:
        judge_clean = raw_bundle.get("judge_clean") or {}
        if isinstance(judge_clean, dict):
            judge_report = (judge_clean.get("content_typo_report") or "").strip()
    if not judge_report:
        judge_report = (raw_bundle.get("content_typo_report") or "").strip()

    return {
        "detector": detector_report,
        "judge": judge_report,
        "final": (final_report or "").strip(),
    }


def create_korean_review_prompt_for_text(korean_text: str) -> str:
    
     # ì›ë¬¸ì„ JSON ë¬¸ìì—´ë¡œ í•œ ë²ˆ ê°ì‹¸ì„œ, ì¸ìš©ë¶€í˜¸/ì¤„ë°”ê¿ˆ/íŠ¹ìˆ˜ë¬¸ìë¥¼ ì•ˆì „í•˜ê²Œ ì „ë‹¬
    safe_text = json.dumps(korean_text, ensure_ascii=False)
    
    prompt = f"""
ë‹¹ì‹ ì€ ê¸°ê³„ì ìœ¼ë¡œ ë™ì‘í•˜ëŠ” **Korean text proofreader**ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ìœ ì¼í•œ ì„ë¬´ëŠ” ì•„ë˜ í•œêµ­ì–´ í…ìŠ¤íŠ¸ì—ì„œ **ê°ê´€ì ì´ê³  ê²€ì¦ ê°€ëŠ¥í•œ ì˜¤ë¥˜ë§Œ** ì°¾ì•„ë‚´ëŠ” ê²ƒì…ë‹ˆë‹¤.
ìŠ¤íƒ€ì¼, ì–´íˆ¬, ìì—°ìŠ¤ëŸ¬ì›€, í‘œí˜„ ê°œì„ , ì˜ë„ ì¶”ë¡ ê³¼ ê°™ì€ ì£¼ê´€ì  íŒë‹¨ì€ ì ˆëŒ€ í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.

ì¶œë ¥ì€ ë°˜ë“œì‹œ ì•„ë˜ 4ê°œì˜ keyë§Œ í¬í•¨í•˜ëŠ” **ë‹¨ì¼ JSON ê°ì²´**ì—¬ì•¼ í•©ë‹ˆë‹¤.
- "suspicion_score": 1~5 ì •ìˆ˜
- "content_typo_report": "" (ë¹„ì›Œë‘ê¸° â€” ì˜ì–´ìš© í•„ë“œ)
- "translated_typo_report": í•œêµ­ì–´ ì˜¤ë¥˜ ì„¤ëª… (ì—†ìœ¼ë©´ "")
- "markdown_report": "" (í•­ìƒ ë¹ˆ ë¬¸ìì—´)

ëª¨ë“  ì„¤ëª…ì€ ë°˜ë“œì‹œ **í•œêµ­ì–´ë¡œ** ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
ì˜¤ë¥˜ê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ëª¨ë“  report í•„ë“œëŠ” "" ì—¬ì•¼ í•©ë‹ˆë‹¤.

------------------------------------------------------------
# ğŸš¨ ì ˆëŒ€ ê¸ˆì§€ ê·œì¹™ (Hallucination ë°©ì§€ â€” ë§¤ìš° ì¤‘ìš”)
------------------------------------------------------------
âŒ ì…ë ¥ í…ìŠ¤íŠ¸ì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´Â·êµ¬ì ˆì„ ìƒì„±  
âŒ ì˜ë„Â·ê°ì •Â·ë‚´ìš©ì„ ì¶”ë¡ í•˜ì—¬ ìƒˆë¡œìš´ ë¬¸ì¥ì„ ì œì•ˆ  
âŒ ë¬¸ì¥ì„ ë°”ê¾¸ê±°ë‚˜ ë‹¤ë¥¸ ë§ë¡œ ë°”ê¿” í‘œí˜„  
âŒ ì…ë ¥ë˜ì§€ ì•Šì€ ë‹¨ì–´ë¥¼ ìˆ˜ì • ëŒ€ìƒìœ¼ë¡œ ì§€ëª©  
âŒ ë‚´ìš© ì™œê³¡ ë˜ëŠ” ì˜ë¯¸ì  ë¹„í‰

ì˜¤ì§ â€œì…ë ¥ ë¬¸ìì—´ ì•ˆì— ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” í† í°â€ë§Œ ì¸ìš©í•˜ê³  ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.

ë˜í•œ, "- 'ì›ë¬¸' â†’ 'ìˆ˜ì •ì•ˆ': ..." í˜•ì‹ì—ì„œ 'ì›ë¬¸' ë¶€ë¶„ì€
ë°˜ë“œì‹œ plain_korean ì•ˆì— ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ë¶€ë¶„ ë¬¸ìì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤.

------------------------------------------------------------
# 1. í•œêµ­ì–´ì—ì„œ ë°˜ë“œì‹œ ì¡ì•„ì•¼ í•˜ëŠ” ê°ê´€ì  ì˜¤ë¥˜
------------------------------------------------------------

(A) ì˜¤íƒˆì / ì² ì ì˜¤ë¥˜  
(B) ì¡°ì‚¬Â·ì–´ë¯¸ ì˜¤ë¥˜  
(C) ë‹¨ì–´ ë‚´ë¶€ ë¶ˆí•„ìš”í•œ ê³µë°±  
(D) ë°˜ë³µ ì˜¤íƒ€  
(E) ëª…ë°±í•œ ë„ì–´ì“°ê¸° ì˜¤ë¥˜  
(F) ë¬¸ì¥ë¶€í˜¸ ì˜¤ë¥˜  
   - ë¬¸ì¥ ëì— ì¢…ê²°ë¶€í˜¸ ì—†ìŒ  
   - ë”°ì˜´í‘œ ì§ ë¶ˆì¼ì¹˜  
   - ëª…ë°±íˆ ì˜ëª»ëœ ì‰¼í‘œ  
   - ë¬¸ì¥ ì¤‘ê°„ì˜ ë¶ˆí•„ìš”í•œ ë§ˆì¹¨í‘œ/ì‰¼í‘œ  

[G] ë¬¸ì¥ë¶€í˜¸ ë’¤ ê³µë°± ê·œì¹™ (ì¤‘ìš”)
- ë¬¸ì¥ ëì— ë§ˆì¹¨í‘œ/ë¬¼ìŒí‘œ/ëŠë‚Œí‘œê°€ ìˆê³ , ê·¸ ë’¤ì—ì„œ ìƒˆë¡œìš´ ë¬¸ì¥ì´ ì‹œì‘ë  ê²½ìš°,
  ë¬¸ì¥ë¶€í˜¸ ë’¤ì˜ ê³µë°±ì€ **ì •ìƒì´ë©° ì˜¤íƒ€ê°€ ì•„ë‹ˆë‹¤.**
- ë‹¨ì–´ ë‚´ë¶€ì—ì„œ ë¶ˆí•„ìš”í•œ ê³µë°±(ì˜ˆ: 'í˜ ë¦°ë‹¤', 'ëœ ë‹¤')ë§Œ ì˜¤ë¥˜ë¡œ ì¸ì •í•œë‹¤.

============================================================
# 2. OUTPUT FORMAT (JSON Only)
============================================================
ì˜¤ë¥˜ê°€ ìˆì„ ê²½ìš° í•œ ì¤„ì”© bullet:

"- 'ì›ë¬¸' â†’ 'ìˆ˜ì •ì•ˆ': ì˜¤ë¥˜ ì„¤ëª…"

------------------------------------------------------------
# 3. ê²€ì‚¬í•  í…ìŠ¤íŠ¸
------------------------------------------------------------

ì•„ë˜ëŠ” ê²€ìˆ˜í•  í•œêµ­ì–´ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ JSON ë¬¸ìì—´ë¡œ ì¸ì½”ë”©í•œ ê°’ì…ë‹ˆë‹¤.
ì´ ê°’ì„ ê·¸ëŒ€ë¡œ ë¬¸ìì—´ë¡œ ë³µì›í•˜ì—¬ ê²€ìˆ˜ì— ì‚¬ìš©í•˜ì„¸ìš”.

plain_korean_json: {safe_text}

- plain_korean_json ê°’ì€ JSON ì¸ì½”ë”©ëœ ë¬¸ìì—´ì…ë‹ˆë‹¤.
- ì´ ê°’ì„ ê·¸ëŒ€ë¡œ ë””ì½”ë”©í•œ í…ìŠ¤íŠ¸(plain_korean)ë¥¼ ê¸°ì¤€ìœ¼ë¡œë§Œ
  '- 'ì›ë¬¸' â†’ 'ìˆ˜ì •ì•ˆ': ...' í˜•ì‹ì˜ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
- 'ì›ë¬¸' ë¶€ë¶„ì€ ë°˜ë“œì‹œ plain_korean ì•ˆì— ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ë¶€ë¶„ ë¬¸ìì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤.

ì´ì œ ìœ„ ê·œì¹™ì„ ì§€í‚¤ë©° plain_korean_jsonì— ë‹´ê¸´ í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ê²€ìˆ˜í•˜ì„¸ìš”.
"""
    return prompt


def _review_korean_single_block(korean_text: str) -> Dict[str, Any]:
    """
    âœ… 2íŒ¨ìŠ¤(Detector â†’ Judge) ê¸°ë°˜ í•œêµ­ì–´ ë‹¨ì¼ ë¸”ë¡ ê²€ìˆ˜
    1ì°¨: Detector í”„ë¡¬í”„íŠ¸ë¡œ ê°€ëŠ¥í•œ ë§ì€ ì˜¤ë¥˜ í›„ë³´ë¥¼ ìˆ˜ì§‘
    2ì°¨: Judge í”„ë¡¬í”„íŠ¸ë¡œ ì˜ë¯¸ ë³€ê²½/ìŠ¤íƒ€ì¼ ì œì•ˆ/í™˜ê° ë“±ì„ í•„í„°ë§
    + ê¸°ì¡´ ê·œì¹™ ê¸°ë°˜ í›„ì²˜ë¦¬(drop_lines_not_in_source ë“±)ë¥¼ í•œ ë²ˆ ë” ì ìš©
    """

    # 1ï¸âƒ£ 1ì°¨ íŒ¨ìŠ¤: Detector
    detector_prompt = create_korean_detector_prompt_for_text(korean_text)
    detector_raw = analyze_text_with_gemini(detector_prompt)
    detector_clean = validate_and_clean_analysis(detector_raw)

    draft_report = detector_clean.get("translated_typo_report", "") or ""

    # 2ï¸âƒ£ 2ì°¨ íŒ¨ìŠ¤: Judge
    judge_prompt = create_korean_judge_prompt_for_text(korean_text, draft_report)
    judge_raw = analyze_text_with_gemini(judge_prompt)
    judge_clean = validate_and_clean_analysis(judge_raw)

    # 2ì°¨ ê²°ê³¼ ê¸°ì¤€ìœ¼ë¡œ ì ìˆ˜/ë¦¬í¬íŠ¸ ì‚¬ìš©
    score = judge_clean.get("suspicion_score", 1)
    try:
        score = int(score)
    except Exception:
        score = 3

    final_report = judge_clean.get("translated_typo_report", "") or ""

    # 3ï¸âƒ£ ê·œì¹™ ê¸°ë°˜ í›„ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§ ê·¸ëŒ€ë¡œ ìœ ì§€)
    filtered = drop_lines_not_in_source(
        korean_text,
        final_report,
    )
    filtered = drop_false_korean_period_errors(filtered)
    filtered = drop_false_whitespace_claims(korean_text, filtered)
    filtered = ensure_final_punctuation_error(korean_text, filtered)
    filtered = ensure_sentence_end_punctuation(korean_text, filtered)
    filtered = dedup_korean_bullet_lines(filtered)
    filtered = drop_lines_not_in_source(korean_text, filtered)  # í•œ ë²ˆ ë” ê²€ì¦

    # 4ï¸âƒ£ raw ë²ˆë“¤ êµ¬ì„± (UI í˜¸í™˜ + ë””ë²„ê·¸ìš© ì •ë³´ í¬í•¨)
    raw_bundle = {
        "mode": "two_pass_single",
        # UIê°€ ê·¸ëŒ€ë¡œ ì“¸ ìˆ˜ ìˆë„ë¡ ìƒìœ„ ìš”ì•½ê°’ë„ ë„£ì–´ë‘ 
        "suspicion_score": score,
        "translated_typo_report": final_report,
        # ë””ë²„ê·¸ìš© ìƒì„¸ ë‹¨ê³„ ì •ë³´
        "detector_raw": detector_raw,
        "detector_clean": detector_clean,
        "judge_raw": judge_raw,
        "judge_clean": judge_clean,
        "initial_report_from_detector": draft_report,
        "final_report_before_rule_postprocess": final_report,
    }

    return {
        "score": score,
        "content_typo_report": "",          # í•œêµ­ì–´ íƒ­ì—ì„œëŠ” ì‚¬ìš© ì•ˆ í•¨
        "translated_typo_report": filtered, # ê·œì¹™ ê¸°ë°˜ í›„ì²˜ë¦¬ê¹Œì§€ ì ìš©ëœ ìµœì¢… ë¦¬í¬íŠ¸
        "markdown_report": "",
        "raw": raw_bundle,
    }

def review_korean_text(korean_text: str) -> Dict[str, Any]:
    """
    í•œêµ­ì–´ í…ìŠ¤íŠ¸ ê²€ìˆ˜ (chunk ì§€ì› ë²„ì „)

    - í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ ì§§ìœ¼ë©´: ê¸°ì¡´ single block ë¡œì§ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    - í…ìŠ¤íŠ¸ê°€ ê¸¸ë©´: ì—¬ëŸ¬ chunkë¡œ ë‚˜ëˆˆ ë’¤, ê° chunkë¥¼ ê°œë³„ ê²€ìˆ˜í•´ì„œ
      ë¦¬í¬íŠ¸ë¥¼ í•©ì³ì„œ ë°˜í™˜
    """
    # 1) chunking
    chunks = split_korean_text_into_chunks(korean_text, max_len=MAX_KO_CHUNK_LEN)

    # chunkê°€ 1ê°œë©´ ê¸°ì¡´ ë¡œì§ ê·¸ëŒ€ë¡œ
    if len(chunks) == 1:
        return _review_korean_single_block(korean_text)

    # 2) ì—¬ëŸ¬ chunkë¥¼ ìˆœì°¨ ê²€ìˆ˜
    merged_report_lines: List[str] = []
    raw_list: List[Dict[str, Any]] = []
    max_score = 1

    for idx, chunk in enumerate(chunks, start=1):
        res = _review_korean_single_block(chunk)

        score = res.get("score", 1) or 1
        max_score = max(max_score, score)

        report = (res.get("translated_typo_report") or "").strip()
        if report:
            # í•„ìš”í•˜ë©´ chunk ë²ˆí˜¸ë¥¼ êµ¬ë¶„ìš© í—¤ë”ë¡œ ë‹¬ì•„ì¤„ ìˆ˜ ìˆìŒ
            merged_report_lines.append(f"# [ë¸”ë¡ {idx}]")
            merged_report_lines.append(report)

        raw_list.append({
            "index": idx,
            "text": chunk,
            "raw": res.get("raw", {}),
            "score": score,
        })

    merged_report = "\n".join(merged_report_lines).strip()

    # ë¦¬í¬íŠ¸ê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ scoreë¥¼ 1ë¡œ í†µì¼
    if not merged_report:
        max_score = 1
    elif max_score <= 1:
        max_score = 3  # ë­”ê°€ ë³´ê³ ëŠ” ìˆëŠ”ë° scoreê°€ 1ì¸ ê²½ìš° ê¸°ë³¸ 3ìœ¼ë¡œ ì˜¬ë¦¬ëŠ” ê²ƒë„ ê°€ëŠ¥

    # rawì—ëŠ” chunkë³„ ì •ë³´ ì „ì²´ë¥¼ ë¬¶ì–´ì„œ ë„£ì–´ë‘”ë‹¤
    raw_bundle = {
        "mode": "chunked",
        "chunk_count": len(chunks),
        "chunks": raw_list,
        "suspicion_score": max_score,  # âœ… ì¶”ê°€
    }


    return {
        "score": max_score,
        "content_typo_report": "",              # í•œêµ­ì–´ íƒ­ì—ì„œëŠ” ì‚¬ìš© ì•ˆ í•˜ë¯€ë¡œ ë¹„ì›Œë‘ 
        "translated_typo_report": merged_report,
        "markdown_report": "",
        "raw": raw_bundle,
    }


# -------------------------------------------------
# 1-B. ì˜ì–´ ë‹¨ì¼ í…ìŠ¤íŠ¸ ê²€ìˆ˜ í”„ë¡¬í”„íŠ¸ + ë˜í¼
# -------------------------------------------------
def create_english_detector_prompt_for_text(english_text: str) -> str:
    """
    1ì°¨ íŒ¨ìŠ¤: Detector
    - ê°€ëŠ¥í•œ ë§ì€ 'ì ì¬ì  ì˜¤ë¥˜ í›„ë³´'ë¥¼ ì°¾ì•„ë‚´ëŠ” ì—­í•  (ê³¼ê²€ì¶œ ì•½ê°„ í—ˆìš©)
    """
    safe_text = json.dumps(english_text, ensure_ascii=False)

    prompt = f"""
You are the first-pass **English text proofreader (Detector)**.

Your job is to detect **as many potential objective errors as possible** in the given English text.
You may slightly over-detect (allow some false positives), because a second-pass Judge will filter them.

Your response MUST be a single JSON object with EXACTLY these keys:
- "suspicion_score": integer 1~5
- "content_typo_report": string
- "translated_typo_report": ""   (keep empty, not used here)
- "markdown_report": ""          (keep empty)

Requirements for "content_typo_report":
- It MUST be a newline-joined list of bullet lines.
- Each line MUST follow this exact format (in Korean):

  - 'ì›ë¬¸' â†’ 'ìˆ˜ì •ì•ˆ': ì˜¤ë¥˜ ì„¤ëª…

- All explanations MUST be written in Korean.
- 'ì›ë¬¸' MUST be an exact substring of the original English text (after decoding).

The types of errors you should detect widely in this Detector pass:

- English spelling mistakes
- Split-word errors: "under stand" â†’ "understand", "s imp le" â†’ "simple"
- AI context "Al" (A + small L) that should be "AI" (artificial intelligence)
- Capitalization errors (sentence start, "i" instead of "I", proper nouns)
- Clear duplicate words ("the the")
- Obvious punctuation problems (missing final punctuation, ",." / ".." etc.)

------------------------------------------------------------
# Input: English text (JSON string)
------------------------------------------------------------

plain_english_json: {safe_text}

- Decode plain_english_json to obtain plain_english.
- In each bullet line "- 'ì›ë¬¸' â†’ 'ìˆ˜ì •ì•ˆ': ì„¤ëª…",
  'ì›ë¬¸' MUST be a substring of plain_english.

Now, carefully detect as many *potential* objective errors as possible,
and output them in "content_typo_report" following the format above.
"""
    return prompt


def create_english_judge_prompt_for_text(english_text: str, draft_report: str) -> str:
    """
    2ì°¨ íŒ¨ìŠ¤: Judge
    - Detectorê°€ ë§Œë“  í›„ë³´ë“¤ ì¤‘ì—ì„œ 'ì˜ë¯¸ë¥¼ ë°”ê¾¸ì§€ ì•ŠëŠ” ê°ê´€ì  ì˜¤ë¥˜'ë§Œ ë‚¨ê¸°ê³  í•„í„°ë§
    """
    safe_text = json.dumps(english_text, ensure_ascii=False)
    safe_report = json.dumps(draft_report, ensure_ascii=False)

    prompt = f"""
You are the second-pass **English text proofreader (Judge)**.

Your role:
- Given the original English text and a candidate error list (draft_report),
  you MUST **keep only the lines that are objective, safe corrections**,
  and discard everything else.

------------------------------------------------------------
# Input 1: original English text (JSON string)
------------------------------------------------------------
plain_english_json: {safe_text}

- Decode this JSON string to get plain_english.

------------------------------------------------------------
# Input 2: Detector's candidate report (JSON string)
------------------------------------------------------------
draft_report_json: {safe_report}

- draft_report_json is a JSON string of the candidate report.
- When decoded, it is a multi-line string.
- Each line has the format:

  - 'ì›ë¬¸' â†’ 'ìˆ˜ì •ì•ˆ': ì„¤ëª…

------------------------------------------------------------
# Filtering Criteria (ALL must be satisfied to keep a line)
------------------------------------------------------------

1. 'ì›ë¬¸' MUST be an exact substring of plain_english.
2. 'ìˆ˜ì •ì•ˆ' MUST represent an **objective, verifiable correction**, such as:
   - spelling / split-word correction
   - clear capitalization fix
   - obvious punctuation fix (missing final ., ?, !, duplicated punctuation, etc.)
3. You MUST REMOVE any line that:
   - rewrites the sentence for style or naturalness,
   - changes wording in a way that could change meaning,
   - adds or removes content beyond a minimal error fix,
   - is just a stylistic suggestion (better wording, tone, clarity, etc.).
4. If 'ì›ë¬¸' does not appear in plain_english at all, that line MUST be removed.

------------------------------------------------------------
# Output
------------------------------------------------------------

Return EXACTLY ONE JSON object with keys:
- "suspicion_score": integer 1~5 (based on remaining errors)
- "content_typo_report":
    a multi-line string containing ONLY the kept bullet lines
    in the same format "- 'ì›ë¬¸' â†’ 'ìˆ˜ì •ì•ˆ': ì„¤ëª…"
- "translated_typo_report": ""   (leave empty)
- "markdown_report": ""          (leave empty)

If no candidate lines satisfy all criteria, "content_typo_report" MUST be "".
All explanations MUST still be written in Korean.
"""
    return prompt



def create_english_review_prompt_for_text(english_text: str) -> str:
    # ì˜ì–´ ì›ë¬¸ë„ JSON ë¬¸ìì—´ë¡œ ì•ˆì „í•˜ê²Œ ê°ì‹¸ê¸°
    safe_text = json.dumps(english_text, ensure_ascii=False)

    
    prompt = f"""
You are a machine-like **English text proofreader**.
Your ONLY job is to detect **objective, verifiable errors** in the following English text.
You are strictly forbidden from judging tone, style, naturalness, or suggesting alternative phrasing.

Your response MUST be a valid JSON object with exactly these keys:
- "suspicion_score": integer (1~5)
- "content_typo_report": string
- "translated_typo_report": string
- "markdown_report": string

All explanations in the *_report fields MUST be written in **Korean**.
If nothing is wrong, each report field MUST be an empty string "".

------------------------------------------------------------
# 1. RULES FOR ENGLISH OBJECTIVE ERRORS
------------------------------------------------------------

## (A) Split-Word Errors (í•­ìƒ ì˜¤íƒ€ë¡œ ì·¨ê¸‰ â€” ë§¤ìš° ì¤‘ìš”)
If an English word appears with an incorrect internal space,
AND removing the space yields a valid English word,
you MUST treat it as a spelling error.

## (B) Normal English spelling mistakes (MUST detect)
Any token similar to a valid English word (1â€“2 letters swapped/missing) MUST be flagged.

## (C) AI ë¬¸ë§¥ì—ì„œ "Al" â†’ "AI" (í•­ìƒ ì¡ê¸°)
If the surrounding sentence mentions:
model / system / tool / chatbot / LLM / agent / dataset / training / inference
then â€œAlâ€ (A+ì†Œë¬¸ì l) MUST be interpreted as a typo for â€œAIâ€.

## (D) Capitalization Errors
- Sentence starting with lowercase
- Pronoun â€œIâ€ written as â€œiâ€
- Proper nouns not capitalized (london â†’ London)

## (E) Duplicate / spacing errors
- "the the"
- "re turn" â†’ "return"
- "mod el" â†’ "model"

## (F) STRICT punctuation rule â€” avoid false positives
You MUST NOT report a punctuation error if the text already ends with ANY of:
- ".", "?", "!"
- '."' / '!"' / '?"'
- ".â€™" / "!â€™" / "?â€™"

ONLY report a punctuation error if:
- the sentence has NO ending punctuation at all, OR
- a closing quotation mark is missing, OR
- punctuation is clearly malformed (e.g. ",.", ".,", "..", "!!", "??" in a wrong place)

------------------------------------------------------------
# 2. OUTPUT FORMAT
------------------------------------------------------------
You MUST output EXACTLY ONE JSON object (no extra text, no markdown).

Each error line example (in Korean):

"- 'understaning' â†’ 'understanding': 'understaning'ì€ ì² ì ì˜¤íƒ€ì´ë©° 'understanding'ìœ¼ë¡œ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤."


Below is the entire English text encoded as a JSON string.
You MUST decode this JSON string to obtain the original text,
and ONLY use that decoded text as the source for all 'original' spans.

plain_english_json: {safe_text}

- plain_english_json is a JSON-encoded string of the original English text.
- You MUST decode it and use the decoded text (plain_english) as the ONLY source.
- In "- 'ì›ë¬¸' â†’ 'ìˆ˜ì •ì•ˆ': ..." format, 'ì›ë¬¸' MUST be an exact substring of plain_english.

Now, following all the above rules, carefully proofread the text in plain_english_json.
"""
    return prompt


def review_english_text(english_text: str) -> Dict[str, Any]:
    """
    ì˜ì–´ í…ìŠ¤íŠ¸ ê²€ìˆ˜ (2-pass: Detector -> Judge)
    - 1ì°¨ Detector: ì ì¬ì  ì˜¤ë¥˜ í›„ë³´ë¥¼ ë„“ê²Œ ìˆ˜ì§‘
    - 2ì°¨ Judge: ì˜ë¯¸ ë³€ê²½/ìŠ¤íƒ€ì¼ ì œì•ˆ/í™˜ê° ì œê±°
    - + ê·œì¹™ ê¸°ë°˜ í›„ì²˜ë¦¬ (drop_lines_not_in_source, ensure_english_final_punctuation)
    """
    # 1ï¸âƒ£ 1ì°¨ íŒ¨ìŠ¤: Detector
    detector_prompt = create_english_detector_prompt_for_text(english_text)
    detector_raw = analyze_text_with_gemini(detector_prompt)
    detector_clean = validate_and_clean_analysis(
        detector_raw,
        original_english_text=english_text,
    )

    draft_report = detector_clean.get("content_typo_report", "") or ""

    # 2ï¸âƒ£ 2ì°¨ íŒ¨ìŠ¤: Judge
    judge_prompt = create_english_judge_prompt_for_text(english_text, draft_report)
    judge_raw = analyze_text_with_gemini(judge_prompt)
    judge_clean = validate_and_clean_analysis(
        judge_raw,
        original_english_text=english_text,
    )

    score = judge_clean.get("suspicion_score", 1)
    try:
        score = int(score)
    except Exception:
        score = 3
    score = max(1, min(5, score))

    final_report = judge_clean.get("content_typo_report", "") or ""

    # 3ï¸âƒ£ ê·œì¹™ ê¸°ë°˜ í›„ì²˜ë¦¬ (ì˜ì–´ìš©)
    #   - LLMì´ í˜¹ì‹œ ì˜ëª» ì¸ìš©í•œ ë¼ì¸ ì œê±°
    #   - ë§ˆì§€ë§‰ ë¬¸ì¥ ì¢…ê²°ë¶€í˜¸ ê´€ë ¨ ìš”ì•½ ë©”ì‹œì§€ ì¶”ê°€ (ë³´ìˆ˜ì ìœ¼ë¡œ)
    filtered = drop_lines_not_in_source(english_text, final_report)
    filtered = ensure_english_final_punctuation(english_text, filtered)
    filtered = drop_lines_not_in_source(english_text, filtered)  # í•œ ë²ˆ ë” ê²€ì¦

    # 4ï¸âƒ£ raw ë²ˆë“¤ êµ¬ì„± (UI/ë””ë²„ê·¸ìš©)
    raw_bundle = {
        "mode": "two_pass_single_en",
        "suspicion_score": score,
        "content_typo_report": final_report,  # Judge ê²°ê³¼(ë£° ì „)
        "detector_raw": detector_raw,
        "detector_clean": detector_clean,
        "judge_raw": judge_raw,
        "judge_clean": judge_clean,
        "initial_report_from_detector": draft_report,
        "final_report_before_rule_postprocess": final_report,
    }

    return {
        "score": score,
        "content_typo_report": filtered,  # ë£° í›„ì²˜ë¦¬ê¹Œì§€ ëë‚œ ìµœì¢… ë¦¬í¬íŠ¸
        "raw": raw_bundle,
    }


# -------------------------------------------------
# ê³µí†µ: JSON diff / ì œì•ˆ ì¶”ì¶œ
# -------------------------------------------------
def summarize_json_diff(raw: dict | None, final: dict | None) -> str:
    if not isinstance(raw, dict):
        raw = {}
    if not isinstance(final, dict):
        final = {}

    lines = []
    all_keys = sorted(set(raw.keys()) | set(final.keys()))

    for key in all_keys:
        rv = raw.get(key, "<ì—†ìŒ>")
        fv = final.get(key, "<ì—†ìŒ>")
        if rv == fv:
            continue

        rv_str = json.dumps(rv, ensure_ascii=False) if isinstance(rv, (dict, list)) else str(rv)
        fv_str = json.dumps(fv, ensure_ascii=False) if isinstance(fv, (dict, list)) else str(fv)

        lines.append(
            f"- **{key}**\n"
            f"  - raw: `{rv_str}`\n"
            f"  - final: `{fv_str}`"
        )

    if not lines:
        return "ì°¨ì´ê°€ ì—†ìŠµë‹ˆë‹¤. (rawì™€ finalì´ ë™ì¼í•©ë‹ˆë‹¤.)"

    return "\n".join(lines)


def extract_korean_suggestions_from_raw(raw: dict) -> list[str]:
    if not isinstance(raw, dict):
        return []
    collected = []
    fields = [
        raw.get("translated_typo_report", ""),
        raw.get("content_typo_report", ""),
        raw.get("markdown_report", ""),
    ]
    for block in fields:
        if not block:
            continue
        for line in block.split("\n"):
            line = line.strip()
            if not line:
                continue
            if not line.startswith("- "):
                line = f"- {line}"
            collected.append(line)
    return collected


def extract_english_suggestions_from_raw(raw: dict) -> list[str]:
    if not isinstance(raw, dict):
        return []
    collected: list[str] = []
    fields = [
        raw.get("content_typo_report", ""),
        raw.get("translated_typo_report", ""),
        raw.get("markdown_report", ""),
    ]
    for block in fields:
        if not block:
            continue
        for line in block.split("\n"):
            line = line.strip()
            if not line:
                continue
            if not line.startswith("- "):
                line = f"- {line}"
            collected.append(line)
    return collected


# -------------------------------------------------
# 2. Streamlit UI
# -------------------------------------------------
st.set_page_config(
    page_title="AI ê²€ìˆ˜ê¸° (Gemini)",
    page_icon="ğŸ“š",
    layout="wide",
)

st.title("ğŸ“š Delta ì‘ì—…ì Test (Gemini ê¸°ë°˜)")
st.caption("í•œêµ­ì–´/ì˜ì–´ ë‹¨ì¼ í…ìŠ¤íŠ¸ + í•´ì„¤ ì–‘ì‹ ë³€í™˜ (ì˜¤íƒˆì/í˜•ì‹ ìœ„ì£¼, ìŠ¤íƒ€ì¼ ì œì•ˆ ê¸ˆì§€).")

tab_ko, tab_en, tab_pdf, tab_about, tab_debug = st.tabs(
    ["âœï¸ í•œêµ­ì–´ ê²€ìˆ˜", "âœï¸ ì˜ì–´ ê²€ìˆ˜","ğŸ“„ í•´ì„¤ í…ìŠ¤íŠ¸ ì •ë¦¬", "â„¹ï¸ ì„¤ëª…", "ğŸ ë””ë²„ê·¸"]
)

# --- í•œêµ­ì–´ ê²€ìˆ˜ íƒ­ ---
# --- í•œêµ­ì–´ ê²€ìˆ˜ íƒ­ ---
with tab_ko:
    st.subheader("í•œêµ­ì–´ í…ìŠ¤íŠ¸ ê²€ìˆ˜")
    default_ko = "ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ ë¬¸ì¥ ì…ë‹ˆë‹¤, ê·¸ëŠ”.ëŠ” í•™êµì— ê°”ë‹¤,"
    text_ko = st.text_area("í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì…ë ¥", value=default_ko, height=220)

    if st.button("í•œêµ­ì–´ ê²€ìˆ˜ ì‹¤í–‰", type="primary"):
        if not text_ko.strip():
            st.warning("ë¨¼ì € í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        else:
            with st.spinner("AIê°€ í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ê²€ìˆ˜ ì¤‘ì…ë‹ˆë‹¤..."):
                result = review_korean_text(text_ko)
            st.session_state["ko_result"] = result

    if "ko_result" in st.session_state:
        result = st.session_state["ko_result"]
        score = result.get("score", 1)
        raw_json = result.get("raw", {}) or {}

        # ìµœì¢… ë¦¬í¬íŠ¸
        final_report_ko = (result.get("translated_typo_report") or "").strip()

        # 1ì°¨ / 2ì°¨ / ìµœì¢… stageë³„ ë¬¸ìì—´ ì¶”ì¶œ
        stage_reports_ko = get_korean_stage_reports(raw_json, final_report_ko)

        # í™”ë©´ìš© JSON (ìµœì¢… ê¸°ì¤€)
        final_json_display = {
            "ì˜ì‹¬ ì ìˆ˜": score,
            "í•œêµ­ì–´ ê²€ìˆ˜_report": stage_reports_ko["final"],
        }
        raw_json_display = {
            "ì˜ì‹¬ ì ìˆ˜": raw_json.get("suspicion_score"),
            "í•œêµ­ì–´ ê²€ìˆ˜_report": stage_reports_ko["judge"],  # 2ì°¨ Judge ê²°ê³¼
        }

        st.success("í•œêµ­ì–´ ê²€ìˆ˜ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        st.metric("ì˜ì‹¬ ì ìˆ˜ (1~5) 1ì  -> GOOD 5ì  -> BAD", f"{float(score):.2f}")

        # ---------------- í•˜ì´ë¼ì´íŠ¸ ì¹´ë“œ ----------------
        with st.container():
            st.markdown("### ğŸ– ì˜¤ë¥˜ ìœ„ì¹˜ Â· í•˜ì´ë¼ì´íŠ¸")

            stage_choice_ko = st.radio(
                "í•˜ì´ë¼ì´íŠ¸ ê¸°ì¤€ ì„ íƒ",
                ["ìµœì¢…(Final)", "2ì°¨ Judge", "1ì°¨ Detector"],
                horizontal=True,
                key="ko_highlight_mode",
            )

            if stage_choice_ko == "ìµœì¢…(Final)":
                report_for_highlight = stage_reports_ko["final"]
                mode_label = "ìµœì¢…(Final) ê¸°ì¤€"
            elif stage_choice_ko == "2ì°¨ Judge":
                report_for_highlight = stage_reports_ko["judge"]
                mode_label = "2ì°¨ Judge ê¸°ì¤€"
            else:
                report_for_highlight = stage_reports_ko["detector"]
                mode_label = "1ì°¨ Detector ê¸°ì¤€"

            spans_ko = parse_korean_report_with_positions(text_ko, report_for_highlight)

            default_punct_keys = list(PUNCT_GROUPS.keys())
            selected_punct_keys_ko = st.multiselect(
                "ë¬¸ì¥ë¶€í˜¸ ì„ íƒ",
                options=default_punct_keys,
                default=default_punct_keys,
                key="ko_punct_filter",
                help="ì„ íƒí•œ ë¶€í˜¸ë§Œ ìƒ‰ìƒ í‘œì‹œ",
            )

            st.markdown(f"#### ğŸ”¦ {mode_label} í•˜ì´ë¼ì´íŠ¸")
            if spans_ko:
                for span in spans_ko:
                    if span["line"] is None:
                        st.markdown(
                            f"- `{span['original']}` â†’ `{span['fixed']}`: {span['message']}"
                        )
                    else:
                        st.markdown(
                            f"- L{span['line']}, C{span['col']} â€” "
                            f"`{span['original']}` â†’ `{span['fixed']}`: {span['message']}"
                        )
            else:
                st.info(f"{mode_label}ìœ¼ë¡œ í•˜ì´ë¼ì´íŠ¸í•  í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤. ì›ë¬¸ì„ ê·¸ëŒ€ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.")

            view_mode_ko = st.radio(
                "ë³´ê¸° ëª¨ë“œ",
                ["ì˜¤ë¥˜ í•˜ì´ë¼ì´íŠ¸", "ë¬¸ì¥ë¶€í˜¸ë§Œ"],
                horizontal=True,
                key="ko_view_mode_toggle",
            )

            selected_chars_ko = (
                set().union(*(PUNCT_GROUPS[k] for k in selected_punct_keys_ko))
                if selected_punct_keys_ko else set()
            )
            if view_mode_ko == "ì˜¤ë¥˜ í•˜ì´ë¼ì´íŠ¸":
                highlighted_ko = highlight_text_with_spans(
                    text_ko,
                    spans_ko if spans_ko else [],
                    selected_punct_chars=selected_chars_ko,
                )
            else:
                highlighted_ko = highlight_selected_punctuation(text_ko, selected_punct_keys_ko)
            st.markdown(
                f"<div style='background:#f7f7f7; border:1px solid #e5e5e5; border-radius:8px; padding:12px;'>"
                f"<pre style='white-space: pre-wrap; background:transparent; margin:0; font-weight:600;'>{highlighted_ko}</pre>"
                f"</div>",
                unsafe_allow_html=True,
            )

            punct_counts_ko = Counter(ch for ch in text_ko if ch in PUNCT_COLOR_MAP)
            badge_order_ko = [
                (".", "ì¢…ê²°ë¶€í˜¸"),
                ("?", "ë¬¼ìŒí‘œ"),
                ("!", "ëŠë‚Œí‘œ"),
                (",", "ì‰¼í‘œ"),
                ('"', "ìŒë”°ì˜´í‘œ"),
                ("'", "ì‘ì€ë”°ì˜´í‘œ"),
            ]
            badges_ko = []
            for ch, label in badge_order_ko:
                count = punct_counts_ko.get(ch, 0)
                color = PUNCT_COLOR_MAP.get(ch, "#e2e3e5")
                badges_ko.append(
                    f"<span style='background-color: {color}; padding: 2px 6px; border-radius: 4px; margin-right: 6px; display: inline-block;'>{label}: {count}</span>"
                )

            st.markdown(
                f"<div style='border: 1px solid #e9ecef; border-radius: 8px; padding: 10px; background: #f8f9fa; margin-bottom: 6px;'>{''.join(badges_ko)}</div>",
                unsafe_allow_html=True,
            )

            st.caption("â€» ë™ì¼í•œ êµ¬ì ˆì´ ì—¬ëŸ¬ ë²ˆ ë“±ì¥í•˜ëŠ” ê²½ìš°, ì²« ë²ˆì§¸ ìœ„ì¹˜ê°€ í•˜ì´ë¼ì´íŠ¸ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            st.markdown("""
                <small>
                <b>ë¬¸ì¥ë¶€í˜¸ ìƒ‰ìƒ ì•ˆë‚´:</b><br>
                <span style='background-color: #fff3cd; padding: 0 3px;'>.</span> ì¢…ê²°ë¶€í˜¸ (., etc) &nbsp;
                <span style='background-color: #f8d7da; padding: 0 3px;'>?</span> ë¬¼ìŒí‘œ &nbsp;
                <span style='background-color: #f5c6cb; padding: 0 3px;'>!</span> ëŠë‚Œí‘œ &nbsp;
                <span style='background-color: #d1ecf1; padding: 0 3px;'>,</span> ì‰¼í‘œ &nbsp;
                <span style='background-color: #e0f7e9; padding: 0 3px;'>&ldquo;</span> ìŒë”°ì˜´í‘œ &nbsp;
                <span style='background-color: #fce9d9; padding: 0 3px;'>&lsquo;</span> ì‘ì€ë”°ì˜´í‘œ &nbsp;
                <span style='background-color: #d6d8d9; padding: 0 3px;'>; :</span> ê¸°íƒ€ ë¬¸ì¥ë¶€í˜¸
                </small>
                """, unsafe_allow_html=True)

        # ---------------- ê²°ê³¼ ë¹„êµ / ì œì•ˆ ì‚¬í•­ ì¹´ë“œ ----------------
        with st.container():
            st.markdown("### ğŸ“Š ê²°ê³¼ ë¹„êµ Â· ì œì•ˆ")
            col1, col2 = st.columns(2)
            with col1:
                st.markdown("#### âœ… Final JSON (í›„ì²˜ë¦¬ ì ìš©)")
                st.json(final_json_display, expanded=False)
            with col2:
                st.markdown("#### ğŸ§ª Raw JSON (2ì°¨ Judge ê¸°ì¤€)")
                st.json(raw_json_display, expanded=False)

            with st.expander("1ì°¨ Detector JSON (í•„ìš” ì‹œ)", expanded=False):
                st.json(raw_json.get("detector_clean", {}))
            with st.expander("2ì°¨ Judge JSON (í•„ìš” ì‹œ)", expanded=False):
                st.json(raw_json.get("judge_clean", {}))

            st.markdown("### ğŸ›  ìµœì¢… ìˆ˜ì • ì œì•ˆ ì‚¬í•­ (ìµœì¢… ê¸°ì¤€)")
            suggestions = extract_korean_suggestions_from_raw(
                {"translated_typo_report": stage_reports_ko["final"]}
            )
            if not suggestions:
                st.info("ë³´ê³ í•  ìˆ˜ì • ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
            else:
                for s in suggestions:
                    st.markdown(s)



# --- ì˜ì–´ ê²€ìˆ˜ íƒ­ ---
with tab_en:
    st.subheader("ì˜ì–´ í…ìŠ¤íŠ¸ ê²€ìˆ˜")
    default_en = 'This is a simple understaning of the Al model.'
    text_en = st.text_area("English text input", value=default_en, height=220)

    if st.button("ì˜ì–´ ê²€ìˆ˜ ì‹¤í–‰", type="primary"):
        if not text_en.strip():
            st.warning("ë¨¼ì € ì˜ì–´ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        else:
            with st.spinner("AIê°€ ì˜ì–´ í…ìŠ¤íŠ¸ë¥¼ ê²€ìˆ˜ ì¤‘ì…ë‹ˆë‹¤..."):
                result = review_english_text(text_en)
            st.session_state["en_result"] = result

    if "en_result" in st.session_state:
        result = st.session_state["en_result"]
        score = result.get("score", 1)
        raw_json = result.get("raw", {}) or {}

        # ìµœì¢… ë¦¬í¬íŠ¸
        final_report_en = (result.get("content_typo_report") or "").strip()
        stage_reports_en = get_english_stage_reports(raw_json, final_report_en)

        final_json = {
            "ì˜ì‹¬ ì ìˆ˜": score,
            "ì˜ë¬¸ ê²€ìˆ˜_report": stage_reports_en["final"],
        }
        raw_view = {
            "ì˜ì‹¬ ì ìˆ˜": raw_json.get("suspicion_score"),
            "ì˜ë¬¸ ê²€ìˆ˜_report": stage_reports_en["judge"],  # 2ì°¨ Judge
        }

        st.success("ì˜ì–´ ê²€ìˆ˜ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        st.metric("ì˜ì‹¬ ì ìˆ˜ (1~5) 1ì  -> GOOD 5ì  -> BAD", f"{float(score):.2f}")

        # ---------------- í•˜ì´ë¼ì´íŠ¸ ì¹´ë“œ ----------------
        with st.container():
            st.markdown("### ğŸ– ì˜¤ë¥˜ ìœ„ì¹˜ Â· í•˜ì´ë¼ì´íŠ¸")

            view_mode_en = st.radio(
                "í•˜ì´ë¼ì´íŠ¸ ê¸°ì¤€ ì„ íƒ",
                ["ìµœì¢…(Final)", "2ì°¨ Judge", "1ì°¨ Detector"],
                horizontal=True,
                key="en_highlight_mode",
            )

            if view_mode_en == "ìµœì¢…(Final)":
                report_for_highlight = stage_reports_en["final"]
                mode_label_en = "ìµœì¢…(Final) ê¸°ì¤€"
            elif view_mode_en == "2ì°¨ Judge":
                report_for_highlight = stage_reports_en["judge"]
                mode_label_en = "2ì°¨ Judge ê¸°ì¤€"
            else:
                report_for_highlight = stage_reports_en["detector"]
                mode_label_en = "1ì°¨ Detector ê¸°ì¤€"

            spans_en = parse_english_report_with_positions(text_en, report_for_highlight)

            default_punct_keys = list(PUNCT_GROUPS.keys())
            selected_punct_keys_en = st.multiselect(
                "ë¬¸ì¥ë¶€í˜¸ ì„ íƒ",
                options=default_punct_keys,
                default=default_punct_keys,
                key="en_punct_filter",
                help="ì„ íƒí•œ ë¶€í˜¸ë§Œ ìƒ‰ìƒ í‘œì‹œ",
            )

            st.markdown(f"#### ğŸ”¦ {mode_label_en} í•˜ì´ë¼ì´íŠ¸")
            if spans_en:
                for span in spans_en:
                    if span["line"] is None:
                        st.markdown(
                            f"- `{span['original']}` â†’ `{span['fixed']}`: {span['message']}"
                        )
                    else:
                        st.markdown(
                            f"- L{span['line']}, C{span['col']} â€” "
                            f"`{span['original']}` â†’ `{span['fixed']}`: {span['message']}"
                        )
            else:
                st.info(f"{mode_label_en}ìœ¼ë¡œ í•˜ì´ë¼ì´íŠ¸í•  í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤. ì›ë¬¸ì„ ê·¸ëŒ€ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.")

            selected_chars_en = (
                set().union(*(PUNCT_GROUPS[k] for k in selected_punct_keys_en))
                if selected_punct_keys_en else set()
            )
            view_mode_en_toggle = st.radio(
                "ë³´ê¸° ëª¨ë“œ",
                ["ì˜¤ë¥˜ í•˜ì´ë¼ì´íŠ¸", "ë¬¸ì¥ë¶€í˜¸ë§Œ"],
                horizontal=True,
                key="en_view_mode_toggle",
            )
            if view_mode_en_toggle == "ì˜¤ë¥˜ í•˜ì´ë¼ì´íŠ¸":
                highlighted_en = highlight_text_with_spans(
                    text_en,
                    spans_en if spans_en else [],
                    selected_punct_chars=selected_chars_en,
                )
            else:
                highlighted_en = highlight_selected_punctuation(text_en, selected_punct_keys_en)
            st.markdown(
                f"<div style='background:#f7f7f7; border:1px solid #e5e5e5; border-radius:8px; padding:12px;'>"
                f"<pre style='white-space: pre-wrap; background:transparent; margin:0; font-weight:600;'>{highlighted_en}</pre>"
                f"</div>",
                unsafe_allow_html=True,
            )

            punct_counts_en = Counter(ch for ch in text_en if ch in PUNCT_COLOR_MAP)
            badge_order_en = [
                (".", "ì¢…ê²°ë¶€í˜¸"),
                ("?", "ë¬¼ìŒí‘œ"),
                ("!", "ëŠë‚Œí‘œ"),
                (",", "ì‰¼í‘œ"),
                ('"', "ìŒë”°ì˜´í‘œ"),
                ("'", "ì‘ì€ë”°ì˜´í‘œ"),
            ]
            badges_en = []
            for ch, label in badge_order_en:
                count = punct_counts_en.get(ch, 0)
                color = PUNCT_COLOR_MAP.get(ch, "#e2e3e5")
                badges_en.append(
                    f"<span style='background-color: {color}; padding: 2px 6px; border-radius: 4px; margin-right: 6px; display: inline-block;'>{label}: {count}</span>"
                )

            st.markdown(
                f"<div style='border: 1px solid #e9ecef; border-radius: 8px; padding: 10px; background: #f8f9fa; margin-bottom: 6px;'>{''.join(badges_en)}</div>",
                unsafe_allow_html=True,
            )

            st.caption("â€» ë™ì¼í•œ êµ¬ì ˆì´ ì—¬ëŸ¬ ë²ˆ ë“±ì¥í•˜ëŠ” ê²½ìš°, ì²« ë²ˆì§¸ ìœ„ì¹˜ê°€ í•˜ì´ë¼ì´íŠ¸ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            st.markdown("""
                <small>
                <b>ë¬¸ì¥ë¶€í˜¸ ìƒ‰ìƒ ì•ˆë‚´:</b><br>
                <span style='background-color: #fff3cd; padding: 0 3px;'>.</span> ì¢…ê²°ë¶€í˜¸ (., etc) &nbsp;
                <span style='background-color: #f8d7da; padding: 0 3px;'>?</span> ë¬¼ìŒí‘œ &nbsp;
                <span style='background-color: #f5c6cb; padding: 0 3px;'>!</span> ëŠë‚Œí‘œ &nbsp;
                <span style='background-color: #d1ecf1; padding: 0 3px;'>,</span> ì‰¼í‘œ &nbsp;
                <span style='background-color: #e0f7e9; padding: 0 3px;'>&ldquo;</span> ìŒë”°ì˜´í‘œ &nbsp;
                <span style='background-color: #fce9d9; padding: 0 3px;'>&lsquo;</span> ì‘ì€ë”°ì˜´í‘œ &nbsp;
                <span style='background-color: #d6d8d9; padding: 0 3px;'>; :</span> ê¸°íƒ€ ë¬¸ì¥ë¶€í˜¸
                </small>
                """, unsafe_allow_html=True)

        # ê²°ê³¼ ë¹„êµ / ì œì•ˆ ì‚¬í•­ ì¹´ë“œ
        with st.container():
            st.markdown("### ğŸ“Š ê²°ê³¼ ë¹„êµ Â· ì œì•ˆ")
            col1, col2 = st.columns(2)
            with col1:
                st.markdown("#### âœ… Final JSON (í›„ì²˜ë¦¬ ì ìš©)")
                st.json(final_json, expanded=False)
            with col2:
                st.markdown("#### ğŸ§ª Raw JSON (2ì°¨ Judge ê¸°ì¤€)")
                st.json(raw_view, expanded=False)

            st.markdown("#### ğŸ” Raw vs Final ì°¨ì´ ìš”ì•½")
            diff_md_en = summarize_json_diff(raw_view, final_json)
            st.markdown(diff_md_en)

            st.markdown("### ğŸ›  ìµœì¢… ìˆ˜ì • ì œì•ˆ ì‚¬í•­ (ìµœì¢… ê¸°ì¤€)")
            suggestions_en = extract_english_suggestions_from_raw(
                {"content_typo_report": stage_reports_en["final"]}
            )
            if not suggestions_en:
                st.info("ë³´ê³ í•  ìˆ˜ì • ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
            else:
                for s in suggestions_en:
                    st.markdown(s)

            with st.expander("1ì°¨ Detector JSON (í•„ìš” ì‹œ)", expanded=False):
                st.json(raw_json.get("detector_clean", {}))
            with st.expander("2ì°¨ Judge JSON (í•„ìš” ì‹œ)", expanded=False):
                st.json(raw_json.get("judge_clean", {}))


# --- PDF í…ìŠ¤íŠ¸ ì •ë¦¬ íƒ­ ---
with tab_pdf:
    st.subheader("ğŸ“„ ë³µì‚¬í•œ í•´ì„¤ í…ìŠ¤íŠ¸ ì •ë¦¬")
    st.markdown('***í•œ í˜ì´ì§€ ë‚´***ì— ìˆëŠ” í…ìŠ¤íŠ¸ë§Œ ë„£ì–´ì£¼ì„¸ìš”')
    st.caption("PDFì—ì„œ ë³µì‚¬í•œ í…ìŠ¤íŠ¸ë¥¼ ë¶™ì—¬ë„£ê³  ì •ë¦¬ + ì²« ì¤„ ì‚­ì œê¹Œì§€ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    pdf_raw_text = st.text_area(
        "PDFì—ì„œ ë³µì‚¬í•œ ì›ë³¸ í…ìŠ¤íŠ¸",
        height=300,
        key="pdf_input_text",
    )

    colA, colB = st.columns([1, 1])
    with colA:
        auto_trim_pdf = st.checkbox("ì•ë’¤ ê³µë°± ìë™ ì œê±°", value=True, key="pdf_trim")

    with colB:
        run_pdf = st.button("í…ìŠ¤íŠ¸ ì •ë¦¬ ì‹¤í–‰", type="primary", key="pdf_run")

    if run_pdf:
        if not pdf_raw_text.strip():
            st.warning("ë¨¼ì € í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        else:
            text_to_send = pdf_raw_text.strip() if auto_trim_pdf else pdf_raw_text
            with st.spinner("Geminiê°€ í…ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                cleaned_block = restore_pdf_text(text_to_send)
            # âœ… ì •ë¦¬ëœ ê²°ê³¼ë¥¼ ì„¸ì…˜ì— ì €ì¥
            st.session_state["pdf_cleaned"] = cleaned_block

    cleaned_block = st.session_state.get("pdf_cleaned")

    if cleaned_block:
        st.markdown("#### âœ… ì •ë¦¬ëœ í…ìŠ¤íŠ¸")

        # ğŸ”˜ ì—¬ê¸°ì„œ 'ë§¨ ìœ„ ì¤„ ì§€ìš°ê¸°' ë²„íŠ¼
        if st.button("ë§¨ ìœ„ ì¤„ë§Œ ì§€ìš°ê¸°", key="pdf_delete_first_line"):
            st.session_state["pdf_cleaned"] = remove_first_line_in_code_block(cleaned_block)
            st.rerun()

        # ìµœì‹  ìƒíƒœ ë³´ì—¬ì£¼ê¸°
        st.markdown(st.session_state["pdf_cleaned"])


# --- ì„¤ëª… íƒ­ ---
with tab_about:

    st.title("ğŸ“˜ í…ìŠ¤íŠ¸ ìë™ ê²€ìˆ˜ê¸° ì„¤ëª…ì„œ")
    st.caption("ì´ íƒ­ì€ ì „ì²´ ì•±ì˜ êµ¬ì¡°ì™€ ë™ì‘ ë°©ì‹ì„ ì„¤ëª…í•©ë‹ˆë‹¤.")

    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "âœ¨ ì•± ì†Œê°œ",
        "âœï¸ í•œêµ­ì–´ ê²€ìˆ˜",
        "âœï¸ ì˜ì–´ ê²€ìˆ˜",
        "ğŸ¯ ì² í•™ & ê·œì¹™"
    ])

    # -------------------------
    # 1) ì•± ì†Œê°œ íƒ­
    # -------------------------
    with tab1:
        st.markdown("""
## âœ¨ ì´ ì•±ì€ ë¬´ì—‡ì„ í•˜ë‚˜ìš”?

ì´ ì•±ì€ **í•œêµ­ì–´/ì˜ì–´ ë‹¨ì¼ í…ìŠ¤íŠ¸ ê²€ìˆ˜ê¸°**ì™€  
**Google Sheets ê¸°ë°˜ ë°°ì¹˜ ê²€ìˆ˜ê¸°**ë¥¼ í¬í•¨í•œ **í†µí•© ìë™ ê²€ìˆ˜ í”Œë«í¼**ì…ë‹ˆë‹¤.

- ìì—°ìŠ¤ëŸ¬ì›€, ë¬¸ì²´, í‘œí˜„ ê°œì„  ë“± **ì£¼ê´€ì  ìˆ˜ì •ì€ ì „í˜€ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.**  
- ì˜¤ì§ **ê°ê´€ì ìœ¼ë¡œ ê²€ì¦ ê°€ëŠ¥í•œ ì˜¤ë¥˜ë§Œ** ê²€ì¶œí•©ë‹ˆë‹¤.  
- ëª¨ë“  ê²€ìˆ˜ëŠ” **JSON-only ì‘ë‹µ + í›„ì²˜ë¦¬ ì•ˆì •í™” ë¡œì§** ê¸°ë°˜ìœ¼ë¡œ ì‘ë™í•˜ì—¬  
  ì˜¤íƒ(False Positive)ê³¼ ëˆ„ë½ì„ ìµœì†Œí™”í•©ë‹ˆë‹¤.

---
""")

    # -------------------------
    # 2) í•œêµ­ì–´ ê²€ìˆ˜ íƒ­
    # -------------------------
    with tab2:
        st.markdown("""
# âœï¸ í•œêµ­ì–´ ê²€ìˆ˜ (Korean Proofreading)

## ğŸ” ê¸°ëŠ¥ ê°œìš”
í•œêµ­ì–´ í…ìŠ¤íŠ¸ì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ **í˜•ì‹ì Â·ëª…ë°±í•œ ì˜¤ë¥˜**ë§Œ ê²€ì¶œí•©ë‹ˆë‹¤:

**ê²€ì¶œí•˜ëŠ” ì˜¤ë¥˜**
- ì˜¤íƒˆì / ë°˜ë³µ ë¬¸ì  
- ì¡°ì‚¬Â·ì–´ë¯¸ ì˜¤ë¥˜  
- ëª…ë°±í•œ ë„ì–´ì“°ê¸° ì˜¤ë¥˜  
- ë¬¸ì¥ë¶€í˜¸ ì˜¤ë¥˜  
  - ì¢…ê²°ë¶€í˜¸ ëˆ„ë½  
  - ë”°ì˜´í‘œ ì§ ë¶ˆì¼ì¹˜  
  - ì´ìƒí•œ ì‰¼í‘œÂ·ë§ˆì¹¨í‘œ  
- (ì˜µì…˜) ë‹¨ì–´ ë‚´ë¶€ ë¶„ë¦¬ ì˜¤ë¥˜ (`ëœ ë‹¤` â†’ `ëœë‹¤`)

**ê²€ì¶œí•˜ì§€ ì•ŠëŠ” í•­ëª©**
- ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„ ë³€ê²½  
- ì˜ë¯¸ê°€ ë‹¬ë¼ì§ˆ ê°€ëŠ¥ì„±ì´ ìˆëŠ” ìˆ˜ì •  
- ë¬¸ì¥ ì¬ì‘ì„± ìˆ˜ì¤€ì˜ êµì •  
- escape/markdown ê¸°ë°˜ ê°€ì§œ ì˜¤ë¥˜  

---

## ğŸ§  ì‘ë™ ë°©ì‹

1. **í•œêµ­ì–´ ì „ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±**  
   - "ì›ë¬¸ ì˜ë¯¸ ë³´ì¡´" ì›ì¹™ì„ ê°•í•˜ê²Œ ëª…ì‹œ  
   - ì˜ˆì‹œ í† í° ì¶œë ¥ ê¸ˆì§€  
2. **Gemini(JSON mode, temperature=0)** í˜¸ì¶œ  
3. **í›„ì²˜ë¦¬ ë‹¨ê³„**  
   - ìŠ¤íƒ€ì¼ ì œì•ˆ ì œê±°  
   - ì¡´ì¬í•˜ì§€ ì•ŠëŠ” 'ì›ë¬¸' ê¸°ë°˜ ìˆ˜ì • ì œê±°  
   - escape ê¸°ë°˜ ì˜¤ë¥˜ ì œê±°  
   - ì¢…ê²°ë¶€í˜¸Â·ë”°ì˜´í‘œ ê´€ë ¨ ì˜¤íƒ ì œê±°  
   - plain / markdown ì˜¤ë¥˜ ë¶„ë¦¬  
4. **ìµœì¢… ì¶œë ¥**  
   - suspicion_score (1~5)  
   - translated_typo_report  
   - raw vs final JSON ë¹„êµ ê°€ëŠ¥

---

## ğŸ§ª 2-íŒ¨ìŠ¤ êµ¬ì¡° (Detector â†’ Judge)
- **1ì°¨ Detector**: ê°€ëŠ¥í•œ ë§ì€ ì˜¤ë¥˜ í›„ë³´ë¥¼ ë„“ê²Œ íƒì§€ (ì•½ê°„ ê³¼ê²€ì¶œ í—ˆìš©)
- **2ì°¨ Judge**: ì˜ë¯¸ ë³€ê²½/ìŠ¤íƒ€ì¼ ì œì•ˆ/í™˜ê°ì„ í•„í„°ë§í•´ **ê°ê´€ì  ì˜¤ë¥˜ë§Œ ë‚¨ê¹€**
- UIì—ì„œ Detector/Judge/Finalì„ ê°ê° ì„ íƒí•´ í•˜ì´ë¼ì´íŠ¸ì™€ ë¦¬í¬íŠ¸ë¥¼ ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---
""")

    # -------------------------
    # 3) ì˜ì–´ ê²€ìˆ˜ íƒ­
    # -------------------------
    with tab3:
        st.markdown("""
# âœï¸ ì˜ì–´ ê²€ìˆ˜ (English Proofreading)

## ğŸ” ê¸°ëŠ¥ ê°œìš”
ì˜ì–´ í…ìŠ¤íŠ¸ì˜ **ê°ê´€ì  ì˜¤ë¥˜ë§Œ** íƒì§€í•©ë‹ˆë‹¤.

**ê²€ì¶œí•˜ëŠ” ì˜¤ë¥˜**
- ìŠ¤í ë§ ì˜¤ë¥˜  
- split-word ì˜¤ë¥˜ (`wi th`, `o f` ë“±)  
- AI ë¬¸ë§¥ì—ì„œ `Al` â†’ `AI` ì˜¤í‘œê¸°  
- ëŒ€ë¬¸ì ê·œì¹™ ìœ„ë°˜  
- ì¤‘ë³µ ë‹¨ì–´  
- ì¢…ê²°ë¶€í˜¸ ëˆ„ë½  

**ê²€ì¶œí•˜ì§€ ì•ŠëŠ” í•­ëª©**
- ìŠ¤íƒ€ì¼Â·í‘œí˜„ ê°œì„   
- ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œì˜ ì¬ì‘ì„±  
- ë§ˆí¬ë‹¤ìš´/escape ê¸°ë°˜ ì˜¤ë¥˜  

---

## ğŸ§  ì‘ë™ ë°©ì‹

1. **ì˜ì–´ ì „ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±**
2. **Gemini(JSON mode)** í˜¸ì¶œ  
3. **í›„ì²˜ë¦¬**  
   - self-equal ë¼ì¸ ì œê±°  
   - ì›ë¬¸ ë¯¸ì¡´ì¬ í† í° ì œê±°  
   - ê°€ì§œ ì¢…ê²°ë¶€í˜¸ ì˜¤ë¥˜ ì œê±°  
   - ìŠ¤íƒ€ì¼ ì œì•ˆ ì°¨ë‹¨  
4. plain / markdown ì˜¤ë¥˜ ë¶„ë¦¬

**ì¶œë ¥ ìš”ì†Œ**
- suspicion_score  
- content_typo_report  
- raw JSON / final JSON / diff

---
""")
    with tab4:
        st.markdown("""
# âœï¸ í•´ì„¤ í…ìŠ¤íŠ¸ ë³€í™˜

## ğŸ” ê¸°ëŠ¥ ê°œìš”
í•´ì„¤ í…ìŠ¤íŠ¸ë¥¼ **[ì •ë‹µ í•´ì„¤] / [ì˜¤ë‹µ í•´ì„¤]** ì–‘ì‹ì— ë§ê²Œ ë³€í™˜í•©ë‹ˆë‹¤.:

- **[ì¶œì œ ìœ í˜•] ~** ì‚­ì œ ë©ë‹ˆë‹¤.
- ì •ë‹µì¸ ì´ìœ , ë‹µì´ ì•„ë‹Œ ì´ìœ  í˜•ì‹ì€ **[ì •ë‹µ í•´ì„¤] / [ì˜¤ë‹µ í•´ì„¤]** ì–‘ì‹ìœ¼ë¡œ ë³€í™˜ë©ë‹ˆë‹¤.
---

## ğŸ§  ì‘ë™ ë°©ì‹

1. PDFì—ì„œ OCRí•œ í…ìŠ¤íŠ¸ë¥¼ ë„£ì–´ì¤ë‹ˆë‹¤.
2. í…ìŠ¤íŠ¸ ì •ë¦¬ ì‹¤í–‰ ë²„íŠ¼ì„ í´ë¦­í•´ì¤ë‹ˆë‹¤.
3. ë³€í™˜ëœ í…ìŠ¤íŠ¸ë¥¼ PDFì™€ ë¹„êµ í›„ ì¼ì¹˜í•  ê²½ìš° ë³µì‚¬í•´ì„œ í•´ì„¤ ì˜ì—­ì— ë„£ì–´ì£¼ì„¸ìš”.


---
""")


    # -------------------------
    # 5) ì „ì²´ ì² í•™ ë° ê·œì¹™ íƒ­
    # -------------------------
    with tab5:
        st.markdown("""
# ğŸ¯ ì „ì²´ ì‹œìŠ¤í…œ ì² í•™ ë° ê·œì¹™

## âœ” ì˜ë¯¸ ë³´ì¡´ ì›ì¹™
ëª¨ë“  ê²€ìˆ˜ ë¡œì§ì€  
**â€œì›ë¬¸ì˜ ì˜ë¯¸ì™€ ì˜ë„ë¥¼ ì ˆëŒ€ ë°”ê¾¸ì§€ ì•ŠëŠ”ë‹¤â€**  
ë¥¼ ìµœìš°ì„  ì›ì¹™ìœ¼ë¡œ í•©ë‹ˆë‹¤.

---

## âœ” Hallucination ë°©ì§€
- `'ì›ë¬¸'`ì€ ë°˜ë“œì‹œ ì‹¤ì œ í…ìŠ¤íŠ¸ì— ì¡´ì¬í•´ì•¼ í•¨  
- JSON-only ì‘ë‹µ  
- ì˜ˆì‹œ í† í°(AAA ë“±) ì¶œë ¥ ê¸ˆì§€  
- ìŠ¤íƒ€ì¼Â·ë¬¸ì²´ ì œì•ˆ ì „ë¶€ ì œê±°  

---

## âœ” ëª©í‘œ
- **ê°ê´€ì  ì˜¤ë¥˜ë§Œ ì •í™•í•˜ê²Œ ê²€ì¶œ**  
- í›„ì²˜ë¦¬ë¡œ ì˜¤íƒ ìµœì†Œí™”  
- plain/markdownì„ ë¶„ë¦¬í•˜ì—¬ ì¶œì²˜ë¥¼ ëª…í™•í•˜ê²Œ í‘œí˜„  

---
""")


